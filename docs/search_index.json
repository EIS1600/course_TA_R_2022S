[["index.html", "DH in AAS - TA with R (2022S) Syllabus Course Details 0.1 Aims, Contents and Method of the Course 0.2 Course Evaluation 0.3 Class Participation 0.4 Homework 0.5 Final Project 0.6 Practice Worksheets (R Notebooks) 0.7 Additional Study Materials 0.8 Software, Tools, &amp; Technologies: 0.9 Submitting Homework: 0.10 Schedule 0.11 Lesson Topics (subject to modifications)", " DH in AAS - TA with R (2022S) Maxim G. Romanov 2022-06-17 Syllabus Course Details Course: 57-528 ONLINE S: Digital Humanities in African and Asian Studies: Text Analysis with R (2022S) / 57-528 ONLINE S: Digitale Geisteswissenschaften in den Afrika-Asien-Studien: Textanalyse mit R Language of instruction: English Meeting time: Fr 12:00-14:00 Additional meeting time: we will need to find a time slot when you can all join and work on your HW assignments together Meeting place: due to COVID, all meetings will be held online via Zoom Meeting link: shared via Slack; other details are available via STiNE Office hours: Fr 14:00-15:00 (on Zoom); if you have any questions, please, post them on Slack Instructor: Dr. Maxim Romanov, maxim.romanov@uni-hamburg.de 0.1 Aims, Contents and Method of the Course The course will offer a practical introduction into R programming language, which is currently one of the most popular choices of humanists interested in investigating humanities problems with computational methods. The focus of the course is text analysis. The course will have three main parts: first, you will be introduced to the basics of R; second, you will learn about main text analysis methods; third, you will work on your own text analysis project. The language of the course is English. Personal computers are required both for in-class work and for your homework (running full versions of either Windows, MacOS, or Linux; unfortunately, neither tablets nor Chrome-based laptops are suitable for this course). No prior programming experience is required, but familiarity with the command line and basic principles of programming will be beneficial. 0.2 Course Evaluation Course evaluation will be a combination of: in-class participation (30%), weekly homework workbooks (25%), DataCamp courses (25%), and the final project (20%). Final projects can be prepared either individually or in groups. 0.2.1 DataCamp This class is supported by DataCamp, the most intuitive learning platform for data science and analytics. Learn any time, anywhere and become an expert in R, Python, SQL, and more. DataCamp’s learn-by-doing methodology combines short expert videos and hands-on-the-keyboard exercises to help learners retain knowledge. DataCamp offers 350+ courses by expert instructors on topics such as importing data, data visualization, and machine learning. They’re constantly expanding their curriculum to keep up with the latest technology trends and to provide the best learning experience for all skill levels. Join over 6 million learners around the world and close your skills gap. You should all get access to DataCamp for the durations of the semester. If you cannot access it, please, contact me as soon as possible. DataCamp 0.3 Class Participation Each class session will consist in large part of practical hands-on exercises led by the instructor. BRING YOUR LAPTOP! We will accommodate whatever operating system you use (Windows, Mac, Linux), but it should be a laptop rather than a tablet. Don’t forget that asking for help counts as participation! 0.4 Homework Just as in research and real life, collaboration is a very good way to learn and is therefore encouraged. If you need help with any assignment, you are welcome to ask a fellow student. If you do work together on homework assignments, then when you submit it please include a brief note (just a sentence or two) to indicate who did what. NB: On submitting homework, see below. 0.5 Final Project Final project will be discussed later. You will have an option to build on what we will be doing in class, but you are most encouraged to pick a topic of your own. The best option will be to work on something relevant to your field of study, your term paper or your thesis. 0.6 Practice Worksheets (R Notebooks) 01_worksheets_familiar-with-r.Rmd.zip 02_worksheets_data-structures.Rmd.zip 03_worksheets_data-manipulation-introduction.Rmd.zip 04_worksheets_data-manipulation-continued.Rmd.zip 05_worksheets_ggplot2-introduction-MGR-mod.Rmd.zip 06_worksheets_functions.Rmd.zip NB: Worksheets 1-6 have been developed by Lincoln Mullen. Source: Lincoln A. Mullen, Computational Historical Thinking: With Applications in R (2018–): http://dh-r.lincolnmullen.com. 0.7 Additional Study Materials Silge, Julia, and David Robinson. Text Mining with R: a Tidy Approach https://www.tidytextmining.com/ Jockers, Matthew L. Text Analysis with R for Students of Literature. New York: Springer, 2014 (shared via Slack) Arnold, Taylor, and Lauren Tilton. Humanities Data in R. New York, NY: Springer Science+Business Media, 2015 (shared via Slack) Healy, Kieran. Data Visualization: A Practical Guide. Princeton University Press, 2018. ISBN: 978-0691181622. http://socviz.co/ Hadley Wickham &amp; Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly, 2017. ISBN: 978-1491910399. https://r4ds.had.co.nz/ Wickham, Hadley. Advanced R, Second Edition. 2 edition. Boca Raton: Chapman and Hall/CRC, 2019. http://adv-r.had.co.nz/ Also check https://bookdown.org/ for more books on R Coding Club R Tutorials (focus on Ecology and Environmental Sciences), https://ourcodingclub.github.io/tutorials.html NB: By the way, this website is also built with R. Check: Yihui Xie. bookdown: Authoring Books and Technical Documents with R Markdown, 2022 https://bookdown.org/yihui/bookdown/ 0.8 Software, Tools, &amp; Technologies: The following is the list of software, applications and packages that we will be using in the course. Make sure to have them installed by the class when we are supposed to use them. The main tools for the course will be the programming language R and RStudio, the premier integrated development environment for R. R: https://cloud.r-project.org/ (choose the version for your operating system!) RStudio: https://rstudio.com/products/rstudio/download/ (RStudio Desktop, Open Source License — the free version) We will also use a variety of packages for R, which we will be installing when necessary. 0.9 Submitting Homework: 0.9.1 Handouts / Workbooks Homework assignments are to be submitted by the beginning of the next class; For the first few classes you must email them to the instructor (as attachments) Later, you will be publishing your homework assignments on your github pages and sending an email to the instructor informing that you have completed your homework and providing a relevant github link. In the subject of your email, please, use the following format: CourseID-LessonID-HW-Lastname-matriculationNumber, for example, if I were to submit homework for the first lesson, my subject header would look like: 070112-L01-HW-Romanov-12435687. DH is a collaborative field, so you are most welcome to work on your homework assignments in groups, however, you must still submit it. That is, if a groups of three works on one assignment, there must be three separate submissions: either emailed from each member’s email and published at each member’s github page. 0.9.2 DataCamp Assignments you are also assigned six four-hour interactive courses on different aspects of R on the educational platform DataCamp (&lt;www.datacamp.com&gt;); i.e., approximately one 4-hour course every two weeks. there are deadlines for each course, which are arranged in terms of your general progress (from introductory to intermediate). while you are enrolled in this course, you can also use any other courses on the DataCamp platform free of charge. I strongly recommend you to take advantage of this opportunity. 0.10 Schedule Location: Online 01 — Fri, 08. Apr. 2022 — 12:00—14:00 XX — Fri, 15. Apr. 2022 — Good Friday 02 — Fri, 22. Apr. 2022 — 12:00—14:00 03 — Fri, 29. Apr. 2022 — 12:00—14:00 04 — Fri, 06. May 2022 — 12:00—14:00 05 — Fri, 13. May 2022 — 12:00—14:00 06 — Fri, 20. May 2022 — 12:00—14:00 XX - Fri, 27. May 2022 - Ascension Weekend 07 — Fri, 03. Jun. 2022 — 12:00—14:00 08 — Fri, 10. Jun. 2022 — 12:00—14:00 09 — Fri, 17. Jun. 2022 — 12:00—14:00 10 — Fri, 24. Jun. 2022 — 12:00—14:00 11 — Fri, 01. Jul. 2022 — 12:00—14:00 12 — Fri, 08. Jul. 2022 — 12:00—14:00 13 — Fri, 15. Jul. 2022 — 12:00—14:00 0.11 Lesson Topics (subject to modifications) [ #01 ] General Introduction: Making Sure Everything Works; Getting to know R [ #02 ] Basics I: Data Structures and Subsetting [ #03 ] Basics II: Data Manipulation &amp; Exloratory Analysis [ #04 ] Basics III: Data Visualization; Functions [ #05 ] Data I: Collecting, Organizing, Creating [ #06 ] Data II: Modeling &amp; Manipulating [ #07 ] Text Analysis Methods I: Words, Ngrams, KWIC, etc. [ #08 ] Text Analysis Methods II: TF-IDF and other Similarity Measures [ #09 ] Text Analysis Methods III: Topic Modeling [ #10 ] Text Analysis Methods IV: Stylometric Analysis [ #11 ] Projects I [ #12 ] Projects II [ #13 ] Projects III "],["general-introduction.html", "1 General Introduction 1.1 Goals 1.2 Software 1.3 Class 1.4 Starting with our first workbook: 1.5 Topics covered 1.6 Reference materials 1.7 Homework 1.8 Common issues with homework 1.9 Submitting homework", " 1 General Introduction 1.1 Goals Install R and R Studio and start working with them R https://www.r-project.org/ R Studio https://www.rstudio.com/ Get to know R Notebooks (R markdown) 1.2 Software R https://www.r-project.org/ R Studio https://www.rstudio.com/ 1.3 Class R Studio Interface Installing libraries (packages) R Notebook elements: combining prose and code Converting R Notebook into different formats 1.3.1 Installing rmarkdown Instructions here: https://bookdown.org/yihui/rmarkdown/installation.html More information on R Markdown: https://rmarkdown.rstudio.com/lesson-1.html https://bookdown.org/yihui/rmarkdown/ 1.4 Starting with our first workbook: Now, download a worksheet file (01_worksheets_familiar-with-r.Rmd.zip). Unzip it and open in RStudio. Let’s work through it! NB: Original worksheets prepared by Lincoln Mullen, GMU (https://dh-r.lincolnmullen.com/worksheets.html) 1.5 Topics covered Values Variables Vectors Built-in functions Using the documentation Data frames Installing and loading packages Simple plots 1.6 Reference materials R Primer (https://dh-r.lincolnmullen.com/primer.html) in: Lincoln A. Mullen, Computational Historical Thinking: With Applications in R (2018–): https://dh-r.lincolnmullen.com. Use the this primer as a quick introduction to the R language, or as a reference for the rest of the course. The original worksheets have been developed by Lincoln Mullen (https://dh-r.lincolnmullen.com/worksheets.html). The ones used in this class might have undergone some changes and relevant adaptations. Your R installation may ‘speak’ your main language. It is nice on one hand, but can be quite inconvenient in class, where the main language is English. You may have to do cast some spells to switch R into English. Possible solutions can be found here: https://stackoverflow.com/questions/13575180/how-to-change-language-settings-in-r/ 1.7 Homework Complete the worksheet Getting familiar with R. Generate the results into HTML or PDF (PDF is a little bit trickier). Start working through the first assigned course in DataCamp (two weeks to complete; requires about 4 hours) Submit your homework as described below. 1.8 Common issues with homework 1.8.1 Tracing errors Errors happen all the time. You will run into errors when you run your code. You will run into error messages when “knitting” your document — as a result, your document will not be generated. To resolve this: It is important to run each chunk of code separately to ensure that they all work. If any of the chunks throw errors, you will not be able to “knit” your documents. When you run into an error, R Markdown panel (usually in the lower left corner of RStudio interface) will tell you in which line the error occurred. You will need to fix it the same way you would in Step 1. 1.8.2 Comments / Commenting out You do not want to constantly keep [re]installing libraries. So, if a library is already installed, you can “comment out” that line. install.packages(&quot;historydata&quot;) install.packages(&quot;dplyr&quot;) For example, the code chunk above should become: #install.packages(&quot;historydata&quot;) #install.packages(&quot;dplyr&quot;) Adding # in front of a line (or a section of a line) turns it into a comment and it will not longer be executed. 1.8.3 Random errors: Think about the following two lines of code. Any issues that you can explain? (You might want to run these lines in R to get some clues) `?median` variable1 &lt;- DigitalHumanities 1.9 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["basics-i-main-data-structures-in-r.html", "2 Basics I: Main data structures in R 2.1 Goals 2.2 Software 2.3 Class 2.4 Reference materials 2.5 Homework 2.6 Submitting homework", " 2 Basics I: Main data structures in R 2.1 Goals Getting to know main data structures in R 2.2 Software the same, with some new libraries: R https://www.r-project.org/ R Studio https://www.rstudio.com/ 2.3 Class Practice worksheet: 02_worksheets_data-structures.Rmd.zip NB: Original worksheets prepared by Lincoln Mullen, GMU (https://dh-r.lincolnmullen.com/worksheets.html) 2.3.1 Topics: Data Structures &amp; Types Data structures Vectors Matrices Data frames Lists Subsetting operations 2.3.2 Additional notes str() compactly displays information about an R object typeof() determines the internal type or storage mode of an R object class() tells you the data structure of an R object: list data.frame matrix vector numeric typeof() &gt; double — a double-precision vector (floats; default numberic vector type) typeof() &gt; integer — a vector of integers character typeof() &gt; character — a vector of strings/characters Checking class/type &amp; Conversion create test &lt;- c(1,2,3,4,5) is.___ tests whether a variable of a ___ class. or type: check the typeof() our test vector; then try test.ch &lt;- as.character(test) and check the type again; can you convert it back into its initial type?). is.___ converts into a ___ class. (___ is either: vector, matrix, data.frame, list) Now try the following commands and check the type and class of the new objects: test.matrix &lt;- as.matrix(test) test.df &lt;- as.data.frame(test) test.list &lt;- as.list(test) test.vector &lt;- as.vector(test.df$test) 2.4 Reference materials Read two chapters from: Wickham, Hadley. Advanced R, Second Edition. 2 edition. Boca Raton: Chapman and Hall/CRC, 2019. http://adv-r.had.co.nz/ Data structures Subsetting For the next class, read the following article (in open access, simply follow the link): Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Additional readings: Read Introduction to Lincoln A. Mullen, Computational Historical Thinking: With Applications in R (2018–): (http://dh-r.lincolnmullen.com/introduction.html) 2.5 Homework Finish your worksheet and submit your HW as described below. 2.6 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["basics-ii-data-manipulation-exloratory-analysis.html", "3 Basics II: Data Manipulation &amp; Exloratory Analysis 3.1 Goals 3.2 Software 3.3 Class 3.4 Reference materials 3.5 Homework 3.6 Submitting homework", " 3 Basics II: Data Manipulation &amp; Exloratory Analysis 3.1 Goals Getting to know the basics of working with data: manipulating data, basic techniques of exploratory analysis 3.2 Software the same, with some new libraries: R https://www.r-project.org/ R Studio https://www.rstudio.com/ 3.3 Class Practice worksheets: 03_worksheets_data-manipulation-introduction.Rmd.zip 04_worksheets_data-manipulation-continued.Rmd.zip NB: Original worksheets prepared by Lincoln Mullen, GMU (https://dh-r.lincolnmullen.com/worksheets.html) 3.3.1 Topics Selecting columns (select()) Filtering rows (filter()) Creating new columns (mutate()) Sorting columns (arrange()) Split-apply-combine (group_by()) Summarizing or aggregating data (summarize()) Data joining with two table verbs (left_join() et al.) Data reshaping (spread() and gather()) 3.4 Reference materials Consult relevant chapters from: Healy, Kieran Data Visualization: A Practical Guide. Princeton University Press, 2018. ISBN: 978-0691181622. http://socviz.co/ Hadley Wickham &amp; Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly, 2017. ISBN: 978-1491910399. https://r4ds.had.co.nz/ Wickham, Hadley. Advanced R, Second Edition. 2 edition. Boca Raton: Chapman and Hall/CRC, 2019. http://adv-r.had.co.nz/ 3.5 Homework Finish your worksheet and submit your HW as described below. Additional: if you’d like more practice, you can use swirl library: To install: install.packages(\"swirl\") To run: library(swirl) Then: swirl() it will offer you a set of interactive exercises similar to DataCamp. 3.6 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["basics-iii-data-visualization-functions.html", "4 Basics III: Data Visualization; Functions 4.1 Goals 4.2 Software 4.3 Class 4.4 Topics 4.5 Reference materials 4.6 Homework 4.7 Submitting homework", " 4 Basics III: Data Visualization; Functions 4.1 Goals Getting to know the basics of data visualization; writing simple functions (reusable, packaged code) 4.2 Software the same, with some new libraries: R https://www.r-project.org/ R Studio https://www.rstudio.com/ 4.3 Class Practice worksheets: 05_worksheets_ggplot2-introduction-MGR-mod.Rmd.zip 06_worksheets_functions.Rmd.zip NB: Original worksheets prepared by Lincoln Mullen, GMU (https://dh-r.lincolnmullen.com/worksheets.html) 4.4 Topics Basics of using ggplot2 “grammar” of graphics Basic geoms in ggplot2 Histogram Lines Bar plots Faceting Labels Create your own plots Writing your own functions Explanation of functions Function calls Function definition 4.5 Reference materials Consult relevant chapters from: Healy, Kieran Data Visualization: A Practical Guide. Princeton University Press, 2018. ISBN: 978-0691181622. http://socviz.co/ Hadley Wickham &amp; Garrett Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly, 2017. ISBN: 978-1491910399. https://r4ds.had.co.nz/ Wickham, Hadley. Advanced R, Second Edition. 2 edition. Boca Raton: Chapman and Hall/CRC, 2019. http://adv-r.had.co.nz/ 4.6 Homework Finish your worksheet and submit your HW as described below. Additional: if you’d like more practice, you can use swirl library: To install: install.packages(\"swirl\") To run: library(swirl) Then: swirl() it will offer you a set of interactive exercises similar to DataCamp. 4.7 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["data-i-collecting-organizing-creating.html", "5 Data I: Collecting, Organizing, Creating 5.1 Goals 5.2 Software 5.3 In Class I: Theoretical and Conceptual 5.4 In Class II: Practical 5.5 OCR in R 5.6 Reference Materials: 5.7 Homework 5.8 Submitting homework", " 5 Data I: Collecting, Organizing, Creating 5.1 Goals Getting to know the basics of working with data: collecting, creating, organizing. 5.2 Software R OCR Engines (https://www.onlineocr.net/) OCR can also be done directly in R (requires Tesseract installed) Excel, Google Spreadsheets, or any other alternative 5.3 In Class I: Theoretical and Conceptual 5.3.1 Ways of obtaining data Reusing already produced data One may require to mold data into a more fitting structure. Creating one’s own dataset Digitizing data from printed and/or hand-written sources 5.3.2 Main format Relational databases or Tables/Spreadsheets (tabular data)? Tabular format: tables; spreadsheets; CSV/TSV files. Unique identifiers: tables with different data can be connected via unique identifiers Note: A relational database (rDB) is a collection of interconnected tables. Tables in an rDB are connected with each other via unique identifiers which are usually automatically created by the database itself when new data is added. One can maintain interconnected tables without creating a rDB: Open Linked Data Example: Table of the growth of cities. One table includes information on population over time; Another table includes coordinates of the cities from the dataset. It is more efficient and practical (reducing error rate from typos) to work on these tables separately, and connect them via unique identifiers of cities which are used in both tables. 5.3.2.1 Note on the CSV/TSV format CSV stands for comma-separated values; TSV — for tab-separated values. Below is an examples of a CSV format. Here, the first line is the header, which provides the names of columns; each line is a row, while columns are separated with , commas. city,growth_from_2000_to_2013,latitude,longitude,population,rank,state New York,4.8%,40.7127837,-74.0059413,8405837,1,New York Los Angeles,4.8%,34.0522342,-118.2436849,3884307,2,California Chicago,-6.1%,41.8781136,-87.6297982,2718782,3,Illinois Houston,11.0%,29.7604267,-95.3698028,2195914,4,Texas Philadelphia,2.6%,39.9525839,-75.1652215,1553165,5,Pennsylvania TSV is a better option than a CSV, since TAB characters (\\t) are very unlikely to appear in values. Neither TSV not CSV are good for preserving new line characters (\\n)—or, in other words, text split into multiple lines/paragraphs. As a workaround, one can convert \\n into some unlikely-to-occur character combination (for example, ;;;), which would be easy to restore into \\n later, if necessary. 5.3.3 Basic principles of organizing data: Tidy Data 5.3.3.1 Tidy Data Each variable is in its own column Each observation is in its own row Each value is in its own cell 5.3.3.2 Clean Data Column names and row names are easy to use and informative. In general, it is a good practice to avoid spaces and special characters. Good example: western_cities Alternative good example: WesternCities Bad example: Western Cities (only the largest) Obvious mistakes in the data have been removed Date format: YYYY-MM-DD is the most reliable format. Any thoughts why? There should be no empty cells: If you have them, it might be that your data is not organized properly. If your data is organized properly, NA must be used as an explicit indication that data point is not available. Each cell must contain only one piece of data. Variable values must be internally consistent Be consistent in coding your values: M and man are different values computationally, but may be the same in the dataset; Keep track of your categories: a document where all codes used in the data set are explained. Preserve original values If you are working with a historical dataset, it might be inconsistent. For example, distances between cities are given in different formats: days of travel, miles, farsaḫs/parasangs, etc.). Instead of replacing original values, it is better to create an additional column, where this information will be homogenized according to some principle. Keeping original data will allow to honogenize data in multiple ways (example: day of travel). Clearly differentiate between the original and modified/modelled values. The use of suffixes can be convenient: Distance_Orig vs Distance_Modified. Most of editing operations should be performed in software other than R; any spreadsheet program will work, unless it cannot export into CSV/TSV format. Keep in mind that if you prepare your data in an Excel-like program, rich formatting (like manual highlights, bolds, and italics) is not data and it will be lost, when you export your data into CSV/TSV format. Note: It might be useful, however, to use rule-based highlighting in order, for example, to identify bad values that need to be fixed. Back up your data! http://github.com is a great place for this, plus it allows to work collaboratively. Google spreadsheets is a decent alternative, but it lacks version control and detailed tracking of changes. 5.4 In Class II: Practical 5.4.1 Morris Dataset: the East Vs. the West War-making capacity since 4000 BCE (in social development points). Data source: Morris, Ian. 2013. The Measure of Civilization: How Social Development Decides the Fate of Nations. Princeton: Princeton University Press. 5.4.1.1 Difficulty: Easy Digitize “War-making capacity since 4000 BCE” from this file Morris_2013_Combined.pdf. Fix the dataset so that it conforms to the principles of tidy data. What should be corrected? Load the data set into R Graph chronological changes in war-making capacities for the East and the West. When the East was in the lead? When the West was in the lead? How can you determine that? (Hint: review logical operators and vector comparison). 5.4.1.2 Difficulty: More complicated Digitize “Maximum Settlement Sizes” from this file Morris_2013_Combined.pdf. Fix the dataset so that it conforms to the principles of tidy data. What should be corrected? The datasets for the East and the West are separate. What would be your strategies to combine them? Graph chronological changes in war-making capacities for the East and the West. When the East was in the lead? What were the most prominent settlements? When the West was in the lead? What were the most prominent settlements? How can you determine that? (Hint: review logical operators and vector comparison). 5.5 OCR in R As was noted above, we can use R to OCR text in PDFs and images. The following libraries will be necessary. library(pdftools) library(tidyverse) library(tesseract) library(readr) This code we can use to OCR individual PNG files. text &lt;- tesseract::ocr(pathToPNGfile, engine = tesseract(&quot;eng&quot;)) readr::write_lines(text, str_replace(pathToPNGfile, &quot;.png&quot;, &quot;.txt&quot;)) This code can be used to process entire PDFs: imagesToProcess &lt;- pdftools::pdf_convert(pathToPDFfile, dpi = 600) text &lt;- tesseract::ocr(imagesToProcess, engine = tesseract(&quot;eng&quot;)) readr::write_lines(text, str_replace(pathToPDFfile, &quot;.pdf&quot;, &quot;.txt&quot;)) NB: I had issues running pdftools on Mac. Make sure that you install additional required tools for it. For more details, see: https://github.com/ropensci/pdftools. More details on how to use Tesseract with R you can find here: https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html 5.6 Reference Materials: Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10. (The article in open access) Check these slides: A. Ginolhac, E. Koncina, R. Krause. Principles of Tidy Data: tidyr https://lsru.github.io/tv_course/lecture05_tidyr.html (Also check their other lectures/slides: ) Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. 5.6.1 Additional Morris, Ian. 2013. The Measure of Civilization: How Social Development Decides the Fate of Nations. Princeton: Princeton University Press. Note: This book is a methodological companion to: Morris, Ian. 2010. Why the West Rules—for Now: The Patterns of History, and What They Reveal about the Future. New York: Farrar, Straus and Giroux. 5.6.2 Additional Readings Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10. (The article in open access) 5.7 Homework Finish your worksheet and submit your HW as described below. Additional: if you’d like more practice, you can use swirl library: To install: install.packages(\"swirl\") To run: library(swirl) Then: swirl() it will offer you a set of interactive exercises similar to DataCamp. 5.8 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["data-ii-modeling-manipulating.html", "6 Data II: Modeling &amp; Manipulating 6.1 Goals: 6.2 Software: 6.3 In Class I: Theoretical and Conceptual 6.4 In Class II: Practical 6.5 Reference Materials 6.6 Homework 6.7 Submitting homework", " 6 Data II: Modeling &amp; Manipulating 6.1 Goals: Getting to know the basics of working with data: modeling, manipulating 6.2 Software: R Excel, Google Spreadsheets, or any other alternative 6.3 In Class I: Theoretical and Conceptual 6.3.1 Ways of modeling data: Categorization “[Modeling is] a continual process of coming to know by manipulating representations.” Willard McCarty, “Modeling: A Study in Words and Meanings,” in Susan Schreibman, Ray Siemens, and John Unsworth, A New Companion to Digital Humanities, 2nd ed. (Chichester, UK, 2016), http://www.digitalhumanities.org/companion/. One of the most common way of modeling data in historical research—joining items into broader categories. Categorization is important because it allows to group items with low frequencies into items with higher frequencies, and through those discern patterns and trends. Additionally, alternative categorizations allow one to test different perspectives on historical data. The overall process is rather simple in terms of technological implementation, but is quite complex in terms of subject knowledge and specialized expertise which is required to make well-informed decisions. For example, let’s say we have the following categories: baker, blacksmith, coppersmith, confectioner, and goldsmith. These can be categorized as occupations; Additionally, blacksmith, coppersmith, and goldsmith can also be categorized as ‘metal industry’, while baker and confectioner, can be categorized as ‘food industry’; Yet even more, one might want to introduce additional categories, such as luxury production to include items like goldsmith and confectioner; and regular production for items like baker, blacksmith, coppersmith. Such categorizations can be created in two different ways, with each having its advantages: first, one can create them as additional columns. This approach will allow to always have the original—or alternative—classifications at hand, which is helpful for re-thinking classifications and creating alternative ones where items will be reclassified differently, based on a different set of assumptions about your subject. second, these can be created in separate files, which might be easier as one does not have to stare at existing classifications and therefore will be less influenced by them in making classification decisions. Additionally, one can use some pre-existing classifications that have already been created in academic literature. These most likely need to be digitized and converted into properly formatted data, as we discussed in the previous lesson. 6.3.2 Normalization This is a rather simple, yet important procedure, which is, on the technical side, very similar to what was described above. In essence, the main goal of normalization is to remove insignificant differences that may hinder analysis. Most common examples would be: bringing information to the same format (e.g., dates, names, etc.) unifying spelling differences It is a safe practice to preserve the initial data, creating normalized data in separate columns (or tables) 6.3.3 Note: Proxies, Features, Abstractions These are the terms that refer to the same idea. The notion of proxies is used in data visualization, that of features—in computer science; that of abstractions—in the humanities (see, for example, Franco Moretti’s Graph, Maps, Trees). The main idea behind these terms is that some simple features of an object can act as proxies to some complex phenomena. For example, Ian Morris uses the size of cities as a proxy to the complexity of social organization. The logic is following: the larger the size of a city, the more complex social, economic and technical organization is required to keep that city functional, therefore it alone can be used as an indicator of the social complexity (or a proxy to the social complexity). While proxies are selected from what is available—usually not much, especially when it comes to historical data—as a way to approach something more complex, it may be argued that abstractions are often arrived to from the opposite direction. We start with an object which is available in its complexity and we reduce its complexity to a more manageable form which—we expect—would prepresent a particular aspect of the initial complex object. Most commonly this is applied to texts in a natural language. For example, in stylometry texts are reduced to freqiency lists of most frequent features, which are expected to represent an authorial fingerprint. The complexity of texts can be reduced in a number of ways: into a list of lemmas (e.g., for topic modeling analysis), frequency lists (e.g., for document distance comparison, such as, for example, stylometry), keyword values (e.g., for identifying texts on a similar topic, using, for example, the TF-IDF method), syntactic structures, ngrams, etc. As you get to practice and experiment more, you will start coming up withyour own ways of creating abstractions depending on your current research questions. 6.4 In Class II: Practical Data for the practical session and homework: Bosker_Data.zip. The data is available in open access and is a supplement to a study (see, Reference Materials). The zipped file includes everything you need for the practical session. Download and unzip (read the article at home!). Note: create a notebook and work through the following questions. Group work is encouraged. Please, explain in one or two sentences what you do in each step, so that your work is also human readable. Please, submit this notebook as your homework. Make sure to name your file in the following manner: 57528-LXX-HW-YourLastName-YourMatriculationNumber.EXT, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number; EXT is the extension of your file — yopu can submit it either as HTML or as a PDF. 6.4.1 SECTION I. Please, provide your answers (2-3 sentences) to the following questions: Can you figure which file contains data? In which format is data? How can we load it into R? (Describe and provide working R code) What is the chronological extent of this data? [easy-ish] What periods can it be divided into? How can we do that? How can we introduce the following categories into this data: [easy] North Africa and Europe? [a bit more complicated] the Ottoman Empire? [a tad tricky] Christiandom and Islamdom? 6.4.2 SECTION II Please, provide your answers as working R code and a couple of sentences to describe how you approach (2-3 sentences) the following problems: What is the chronological extent of this data? [easy-ish] What periods can it be divided into? How can we do that? Can you generate a cumulative graph of population over time, divided into these periods? (Hint: there should be one line for one period and another for another, etc.) How can we introduce the following categories into this data: [easy] North Africa and Europe? Construct comparative graphs of population in North Africa and Europe (similar to what you did with the Morris dataset). Here you will need to sum up population! [a bit more complicated] the Ottoman Empire? When did the Empire had the largest number of cities (based on the data set)? When was its population at the highest? [a tad tricky] Christiandom and Islamdom? What are the largest cities of Islamdom for each reported period? What are the largest western cities of Islamdom between 1000 and 1500 CE? 6.5 Reference Materials Bosker, Maarten, Eltjo Buringh, and Jan Luiten van Zanden. 2012. “From Baghdad to London: Unraveling Urban Development in Europe, the Middle East, and North Africa, 800–1800.” The Review of Economics and Statistics 95 (4): 1418–37. https://doi.org/10.1162/REST_a_00284. Bosker, Maarten, Eltjo Buringh, and Jan Luiten Van Zanden. 2014. “Replication Data for: From Baghdad to London: Unraveling Urban Development in Europe, the Middle East, and North Africa, 800-1800.” Harvard Dataverse. https://doi.org/10.7910/DVN/24747. 6.5.1 Additional Readings Moretti, Franco. 2007. Graphs, Maps, Trees: Abstract Models for Literary History. London - New York: Verso. Moretti, Franco. 2013. Distant Reading. London; New York: Verso. Romanov, Maxim G. 2017. “Algorithmic Analysis of Medieval Arabic Biographical Collections.” Speculum 92 (S1): S226–46. https://doi.org/10.1086/693970. 6.6 Homework Finish your worksheet and submit your HW as described below. Additional: if you’d like more practice, you can use swirl library: To install: install.packages(\"swirl\") To run: library(swirl) Then: swirl() it will offer you a set of interactive exercises similar to DataCamp. 6.7 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["text-analysis-i-basics.html", "7 Text Analysis I: Basics 7.1 Goals 7.2 Preliminaries 7.3 Texts and Text Analysis 7.4 Word Frequencies and Word Clouds 7.5 Word Distribution Plots 7.6 Word Distribution Plots: With Frequencies Over Time 7.7 KWIC: Keywords-in-Context 7.8 Homework 7.9 Submitting homework", " 7 Text Analysis I: Basics 7.1 Goals basic text analysis concepts; word frequencies and word clouds; word distribution plots; kwic: keywords-in-context 7.2 Preliminaries 7.2.1 Data We will use the following text files in this worksheet. Please download them and keep them close to your worksheet. Since some of the files are quite large, you want to download them before loading them into R: The Richmond Dispatch (1862) Star Wars I: The Phantom Menace (script) In order to make loading these files a little bit easier, you can paste the path to where you placed these files into an isolated variable and then reuse it as follows (in other words, make sure that your pathToFiles is the path on your local machine): pathToFiles = &quot;./files/data/&quot; d1862 &lt;- read.delim(paste0(pathToFiles, &quot;dispatch_1862.tsv&quot;), encoding=&quot;UTF-8&quot;, header=TRUE, quote=&quot;&quot;) sw1 &lt;- scan(paste0(pathToFiles, &quot;sw1.md&quot;), what=&quot;character&quot;, sep=&quot;\\n&quot;) The first file is articles from “The Daily Dispatch” for the year 1862. The newspaper was published in Richmond, VA — the capital of the Confederate States (the South) during the American Civil War (1861-1865). The second file is a script of the first episode of Star Wars :). 7.2.2 Libraries The following are the libraries that we will need for this section. Install those that you do not have yet. #install.packages(&quot;tidyverse&quot;, &quot;readr&quot;, &quot;stringr&quot;) #install.packages(&quot;tidytext&quot;, &quot;wordcloud&quot;, &quot;RColorBrewer&quot;&quot;, &quot;quanteda&quot;, &quot;readtext&quot;) # General ones library(tidyverse) library(readr) library(&quot;RColorBrewer&quot;) # Text Analysis Specific library(stringr) library(tidytext) library(wordcloud) library(quanteda) library(readtext) 7.2.3 Functions in R (a refresher) Functions are groups of related statements that perform a specific task, which help breaking a program into smaller and modular chunks. As programs grow larger and larger, functions make them more organized and manageable. Functions help avoiding repetition and makes code reusable. Most programming languages, R including, come with a lot of pre-defined—or built-in—functions. Essentially, all statements that take arguments in parentheses are functions. For instance, in the code chunk above, read.delim() is a function that takes as its arguments: 1) filename (or, path to a file); 2) encoding; 3) specifies that the file has a header; and 4) not using \" as a special character. We can also write our own functions, which take care of sets of operations thet we tend to repeat again and again. Later, take a look at this video by one of the key R developers, and check this tutorial. 7.2.3.1 Simple Function Example: Hypothenuse (From Wikipedia) In geometry, a hypotenuse is the longest side of a right-angled triangle, the side opposite the right angle. The length of the hypotenuse of a right triangle can be found using the Pythagorean theorem, which states that the square of the length of the hypotenuse equals the sum of the squares of the lengths of the other two sides (catheti). For example, if one of the other sides has a length of 3 (when squared, 9) and the other has a length of 4 (when squared, 16), then their squares add up to 25. The length of the hypotenuse is the square root of 25, that is, 5. Let’s write a function that takes lengths of catheti as arguments and returns the length of hypothenuse: hypothenuse &lt;- function(cathetus1, cathetus2) { hypothenuse&lt;- sqrt(cathetus1*cathetus1+cathetus2*cathetus2) print(paste(&quot;In the triangle with catheti of length&quot;, cathetus1, &quot;and&quot;, cathetus2, &quot;, the length of hypothenuse is&quot;, hypothenuse)) return(hypothenuse) } Let’s try a simple example: hypothenuse(3,4) ## [1] &quot;In the triangle with catheti of length 3 and 4 , the length of hypothenuse is 5&quot; ## [1] 5 Let’s try a crazy example: hypothenuse(390,456) ## [1] &quot;In the triangle with catheti of length 390 and 456 , the length of hypothenuse is 600.029999250037&quot; ## [1] 600.03 ###$ More complex one: Cleaning Text Let’s say we want to clean up a text so that it is easier to analyze it: 1) convert everithing to lower case; 2) remove all non-alphanumeric characters; and 3) make sure that there are no multiple spaces: clean_up_text = function(x) { x %&gt;% str_to_lower %&gt;% # make text lower case str_replace_all(&quot;[^[:alnum:]]&quot;, &quot; &quot;) %&gt;% # remove non-alphanumeric symbols str_replace_all(&quot;\\\\s+&quot;, &quot; &quot;) # collapse multiple spaces } Let’s test it now: text = &quot;This is a sentence with punctuation, which mentions Hamburg, a city in Germany.&quot; clean_up_text(text) ## [1] &quot;this is a sentence with punctuation which mentions hamburg a city in germany &quot; 7.3 Texts and Text Analysis We can think of text analysis as means of extracting meaningful information from structured and unstructured texts. As historians, we often do that by reading texts and collecting relevant information by taking notes, writing index cards, summarizing texts, juxtaposing one texts against another, comparing texts, looking into how specific words and terms are used, etc. Doing text analysis computationally we do lots of similar things: we extract information of specific kind, we compare texts, we look for similarities, we look differences, etc. While there are similarities between traditional text analysis, there are of course, also significant differences. One of them is procedural: in computational reading we must explicitly perform every step of our analyses. For example, when we read a sentence, we, sort of, automatically identify the meaningful words — subject, verb, object, etc.; we identify keywords; we parse every word, identifying what part of speech it is, what is its lemma (i.e. its dictionary form, etc.). By doing these steps we re-construct the meaning of the text that we read — but we do most of these steps almost unconsciously, especially if a text is written in our native tongues. In computational analysis, these steps must be performed explicitly (in the order of growing complexity): Tokenization: what we see as a text made of words, the computer sees as a continuous string of characters (white spaces, punctuation and the like are characters). We need to break such strings into discreet objects that computer can understand construe as words. Lemmatization: reduces the variety of forms of the same words to their dictionary forms. Another, somewhat similar procedure is called stemming, which usually means the removal of most common suffixes and endings to get to the stem (or, root) of the word. POS (part-of-speech tagging): this is where we run some NLP tool that identifies the part of speech of each word in our text. Syntactic analysis: is the most complicated procedure, which is also usually performed with some NLP tool, which analyzes syntactic relationships within each sentence, identifying its subject(s), verb(s), object(s), etc. NOTE: NLP: natural language processing; Token: you can think of token as a continuous string of letter characters, as a word as it appears in the text in its inflected forms with possible other attached elements (in Arabic we often have prepositions, articles, pronominal suffixes, which are not part of the word, but attached to it); Lemma: the dictionary form of the word; Stem: a “root” of the word; Some examples: #install.packages(&quot;textstem&quot;) library(textstem) sentence = c( &quot;He tried to open one of the bigger boxes.&quot;, &quot;The smaller boxes did not want to be opened.&quot;, &quot;Different forms: open, opens, opened, opening, opened, opener, openers.&quot; ) The library textstem does lemmatization and stemming, but only for English. Tokenization can be performed with str_split() function — and you can define how you want your string to be split. Tokenization str_split(sentence, &quot;\\\\W+&quot;) ## [[1]] ## [1] &quot;He&quot; &quot;tried&quot; &quot;to&quot; &quot;open&quot; &quot;one&quot; &quot;of&quot; &quot;the&quot; &quot;bigger&quot; ## [9] &quot;boxes&quot; &quot;&quot; ## ## [[2]] ## [1] &quot;The&quot; &quot;smaller&quot; &quot;boxes&quot; &quot;did&quot; &quot;not&quot; &quot;want&quot; &quot;to&quot; ## [8] &quot;be&quot; &quot;opened&quot; &quot;&quot; ## ## [[3]] ## [1] &quot;Different&quot; &quot;forms&quot; &quot;open&quot; &quot;opens&quot; &quot;opened&quot; &quot;opening&quot; ## [7] &quot;opened&quot; &quot;opener&quot; &quot;openers&quot; &quot;&quot; Lemmatization lemmatize_strings(sentence) ## [1] &quot;He try to open one of the big box.&quot; ## [2] &quot;The small box do not want to be open.&quot; ## [3] &quot;Different form: open, open, open, open, open, opener, opener.&quot; Stemming stem_strings(sentence) ## [1] &quot;He tri to open on of the bigger box.&quot; ## [2] &quot;The smaller box did not want to be open.&quot; ## [3] &quot;Differ form: open, open, open, open, open, open, open.&quot; Note: It is often important to ensure that all capital letters are converted into small letters or the other way around; additionally, some normalization procedures may be necessary to reduce orthographic complexities of specific languages (for example, ö &gt; oe in German; simplification of forms of alif in Arabic, etc.). 7.4 Word Frequencies and Word Clouds Let’s load all issues of Dispatch from 1862. library(tidytext) d1862 &lt;- read.delim(paste0(pathToFiles, &quot;dispatch_1862.tsv&quot;), encoding=&quot;UTF-8&quot;, header=TRUE, quote=&quot;&quot;) kable(head(d1862)) id date type header text 1862-06-25_article_1 1862-06-25 article The lines. The lines. On Monday night signal rockets were frequently seen to ascend from our exteme left, important of future events, and from preparations everywhere visible we were led to suppose that transactions of a mementous charactor would have transpired yesterday. but despite every conjecture nothing whatever occurred of importance. Sharp skirmishing is of daily occurrence on our extreme left, but the results have not yet been fully developed. Several wounded in these affairs have arrived in the city and report the enemy unwontedly pugnactous in that direction, although from every indication, we are ted to believe that their increasing appetite for slanghter will be more than fully satisfied are many hours shall have passed over us. A few prisoners were brought into our Lines yesterday and Monday, and from their reports it would seem that no preparations are yet perfected by the Federals for any ” onward to Richmond” movement. It is possible Libby’s warehouse will soon be honored by the arrival of a few hundred of the blue-coated gentry, and their greeting be far otherwise than as conquerors. From the interior of the enemy’s lines we learn from recent arrivals that the depredations of the foe have been frequent, their bearing remarkably imperious and overbearing — violent hands being laid upon every species of property, while aged inbabitants are daily subjected to harsh language and ill manners. Since Gen. Stuart’s visit to their rear the rancor and ill-breeding of Lincoln’s hirelings have been more than usually oppressive, their threats and taunts are increasing as to blackguardism, while the unprotected have no resort but patience and silent endurance. Indeed. Federal cavalry have been particularly active in and around Charles City, we hear; squads are patrolling all the country in search of rebels and the disloyal; but up to the present their anxiety and industry have been of no avail, for the mounted rebols have thus far cleverly cluded all traps and search. The movement of troops has been constant, but with what intent and purpose we have no positive idea. The men are in splendid order — all is enthusiasm, animation, preparation and impatience — there is an oppressive sense of incertitude, however-imaginings of bye-gone tardiness to be again rehearsed perhaps — yet all with one accord yearn for action, and are willing to trust the issue to sinewy arms and valiant hearts. An ominous slience reigus supreme-pickets in the timbor move thoughtfully to and fro without firing an accustomed shot; artillerists lean upon their guns and scan the landscape, indifferent to the seene. All is repose — the fields are moist and green, camp fires glisten and glow in the evening air, and the sun goes down upon the quietest day that we have ever seen at the Lines. The enemy are reported to have been extremely busy in their inhospitable swamp for several nights past, pickets imagine the noise to have arisen from the movements of divisions &amp; c. Whenever the conflict shall begin, however, the sequel will demonstrate that the enemy have devised every means known to ingenuity, cunning, and malice to cause destruction among our columns. They will not trust as heretofore, in numbers, nor will they advance upon the open field and try the guage of battle. All will be left to trape, batteries, felled timber, and obstructed roads, and failing in these, panic will be universal among them, and the slaughter unprecedented.’Tis useless to conjecture a few hours perhaps, will reveal all. 1862-06-25_article_2 1862-06-25 article Stuarts Reconnoissance. Stuarts Reconnoissance. The successful raid of Gen. Stuart upon the rear of the Grand Army still continues to puzzle the Yankees, and they have not yet arrived at a full knowledge of the affair. The fact that something was done; that the Confederate cavalry was really within their lines; that a large amount of property was destroyed; that the United States dragoons were routed, and many of them taken prisoners, begins to break slowly upon their minds. It is still spoken of as the work of guerrilla parties, and a wholesome fear of these same parties has sprung up, and it is singular how many of them can be According to she statements of the correspondents, guerrilla bands are still hanging upon the near of the army, and they are seen in every quarter — sometimes at Hanover, on the Pacunkey, at White House, New Kent, Charles City, and other places. Something must be done to put a stop to such lawless proceedings; for according to the Yankee idea, a guerrilla band lurks behind every bush and in every patch of woodland. A more effectual scare has not been given since the war began. Although the rebel Stuart was finally routed and driven beyond the Chickahominy, the New York Herald says that it must be allowed by all to be one of the most daring acts ever known, and greatly to the credit of the rebels. The great excitement prevailing throughout the army, caused by this brilliant affair, is the general topic of conversation. It is perfectly incomprehensible. 1862-06-25_article_3 1862-06-25 article The Yankees in King George. The Yankees in King George. The depredations of the Yankees in King George and the adjoining counties still continue, and many of the citizens have suffered severely, having their crope destroyed, their cattle stolen, and servants carried off. The following extracts are from a letter written by a lady of King George county giving some facts regarding matters in that section:” When I last wrote to you we thought the Yankees were under good discipline, and that the officers would prevent maranding, but we soon found our mistake. They grow bolder and bolder every day, finding no troops here to oppose them, entirely destroying some farms, and wantonly shooting all the stock, while the influence they exerted over the servante was almost beyond endurance. – The Lieutenant-Colonel of a regiment now here, with 200 men, rode through our quarters, telling the servants they were all free, and as good as their masters; that they must not do any work he had come to fight for their freedom, and, if he could, he would give ten lives for them. He also invited them to come over to the camp, and bring fine horses with them. This discourse had the effect of making all the avallable men go off. Among them our coachman, in whom we had the greatest confidence. They took all our carriages and riding horses, &amp; c. Learning that these were sometimes returned, mama went up to the camp herself. The Colonel was very polite, and gave her an order for the horses, which were restored, but the men we shall never see. I cannot deseribe to you the desolation of Fredericksburg. The town is entirely under the rule of negroes. They go to their masters and say they will not work without wages, and demand large sums for the least service; the Yankees exulting over their impertinences. They are grazing their horses on all the wheat fields, and taking every bushel of corn they can find. You remember the beautiful place of the Seddons. He is in the army, and they are eneamped on the place. No one can imagine what Mrs. S. has endured. I trust our faith will not waiver under any circumstances, and I would give up all for our beloved Confederacy.” 1862-06-25_article_4 1862-06-25 article About McClellan. About McClellan. The ” young Napoleon” had a narrow escape a short time ago of being captured by Stuart and his savalry. The Petersburg Express learns that our forces passed in their march an estate in New Kent county known as Hampstead, where Gen. McClellan has established his headquarters. At one point on the march the Confederates were within six hundred yards of Hampstead, and it has since been assertained that at the time they passed McClellan and Staff were all in the house at dinner. The only troops near were his usual body guard, numbering not more than 100 cavalry. Had Gen. Stuart been aware of the fact the building might have been surrounded and the Yankee General captured, to gether with his whole Staff. We have no doubt that within the next week the young Napoleon will wish he had been captured, for in that case, he might have avoided the thorough thrashing which is in store for him. In the latest news from from the North McClellan’s force is put down at 200, 000 although every one knows he has hardly half that number. A large number of his men are sick, and the health of the army is spoken of by a correspondent as decidedly bad. It has been said in a Yankee paper that their loss in the battle of the Chickahominy in killed wounded, missing, and demoralised amounts to 40, 000. We have no doubt it would take more than that number of men to make the army as effective as before the battle. 1862-06-25_article_5 1862-06-25 article NO HEADER Pleasant quarters. – A Yankee correspondent with Halleck’s army writes:” The troops suffer less from disease than from makes, wood ticks, lizzards, scorpions, and gallinippers. Inseets and reptties are thus classed; One Ezzard equals five scorpious; two scorpions equal one gaillnipper; one gallinipper equals one make; one snake, one gallinipper, two scorpions and one Ezzard equal one wood tick.” 1862-06-25_article_6 1862-06-25 article Death of a brave man. Death of a brave man. Capt. Jame B. Brower, of the Nansemond Cavelry, died at the residence of his brother in Prince George county, a few days since. He had been in command of this spirited envalry company for nearly a year, and had performed much service in the vicinity of Pertamouth and the Albemarie region of North Carolina. His very name was a terror to the Yankees and Union men in the vicinity of Edenton and Elizabeth City. He was a brave man. When his bealth failed he resigned his commond and yielded to death. Capt. Brewer leaves an interesting family at Suffolk. We can quickly check what types of articles are there in those issues. d1862 %&gt;% count(type, sort=T) ## type n ## 1 article 14639 ## 2 orders 3972 ## 3 advert 2023 ## 4 ad-blank 535 ## 5 death 233 ## 6 married 186 ## 7 died 70 ## 8 poem 44 ## 9 order 30 ## 10 letter 27 ## 11 ordered 15 ## 12 entry 7 ## 13 acticle 2 ## 14 notice 2 ## 15 role 2 ## 16 runaway 2 ## 17 article 1 ## 18 25 1 ## 19 Wanted 1 ## 20 adverts 1 ## 21 aritcle 1 ## 22 artcle 1 ## 23 articl 1 ## 24 articler 1 ## 25 aticle 1 ## 26 death, 1 ## 27 married, 1 ## 28 marry 1 ## 29 oders 1 ## 30 oped 1 ## 31 ordinal 1 ## 32 printrun 1 ## 33 ranaway 1 ## 34 simple 1 We can create subsets of articles based on their types. death_d1862 &lt;- d1862 %&gt;% filter(type==&quot;death&quot; | type == &quot;died&quot;) Create subsets for other major types. Describe problems with the data set and how they can be fixed. your answer goes here… Now, let’s tidy them up: to work with this as a tidy dataset, we need to restructure it in the one-token-per-row format, which as we saw earlier is done with the unnest_tokens() function. test_set &lt;- death_d1862 test_set_tidy &lt;- test_set %&gt;% mutate(item_number = cumsum(str_detect(text, regex(&quot;^&quot;, ignore_case = TRUE)))) %&gt;% select(-type) %&gt;% unnest_tokens(word, text) %&gt;% mutate(word_number = row_number()) head(test_set_tidy) ## id date header item_number word word_number ## 1 1862-06-25_died_80 1862-06-25 Died. 1 died 1 ## 2 1862-06-25_died_80 1862-06-25 Died. 1 on 2 ## 3 1862-06-25_died_80 1862-06-25 Died. 1 monday 3 ## 4 1862-06-25_died_80 1862-06-25 Died. 1 june 4 ## 5 1862-06-25_died_80 1862-06-25 Died. 1 23d 5 ## 6 1862-06-25_died_80 1862-06-25 Died. 1 of 6 Stop words is an important concept. In general, this notion refers to the most frequent words/tokens which one might want to exclude from analysis. There are existing lists of stop words that you can find online, and they can work fine for testing purposes. data(&quot;stop_words&quot;) test_set_tidy_clean &lt;- test_set_tidy %&gt;% anti_join(stop_words, by=&quot;word&quot;) head(test_set_tidy_clean) ## id date header item_number word word_number ## 1 1862-06-25_died_80 1862-06-25 Died. 1 died 1 ## 2 1862-06-25_died_80 1862-06-25 Died. 1 monday 3 ## 3 1862-06-25_died_80 1862-06-25 Died. 1 june 4 ## 4 1862-06-25_died_80 1862-06-25 Died. 1 23d 5 ## 5 1862-06-25_died_80 1862-06-25 Died. 1 12 7 ## 6 1862-06-25_died_80 1862-06-25 Died. 1 o&#39;clock 8 For research purposes, it is highly advisable to develop your own stop word lists. The process is very simple: create a frequency list of your tokens/words; arrange them by frequencies in descending order; save top 2-3,000 in a tsv/csv file; open in any table editor; add a new column and tag those words that you want to exclude. For example, 1 – for to exclude; 0 — for to keep. It is convenient to automatically fill the column with some default value (0), and then you can change only those that you want to remove (1). You will see that some words, despite their frequency, might be worth keeping. When you are done, you can load them and use anti_join function to filter your corpus. 7.4.1 Word Frequencies Let’s first count all the words: test_set_tidy %&gt;% count(word, sort = TRUE) %&gt;% head(15) ## word n ## 1 the 9352 ## 2 of 8148 ## 3 and 5875 ## 4 his 3390 ## 5 to 3355 ## 6 in 3272 ## 7 a 2730 ## 8 at 2388 ## 9 on 2331 ## 10 he 1595 ## 11 her 1533 ## 12 was 1211 ## 13 o&#39;clock 1102 ## 14 from 1084 ## 15 this 1084 Now, let’s also remove the stop words: test_set_tidy %&gt;% anti_join(stop_words, by=&quot;word&quot;) %&gt;% count(word, sort = TRUE) %&gt;% head(15) ## word n ## 1 o&#39;clock 1102 ## 2 friends 992 ## 3 residence 952 ## 4 funeral 883 ## 5 aged 763 ## 6 attend 704 ## 7 family 633 ## 8 age 588 ## 9 inst 585 ## 10 died 573 ## 11 invited 569 ## 12 months 512 ## 13 morning 481 ## 14 son 453 ## 15 death 406 7.4.2 Wordclouds Wordclouds can be an efficient way to visualize most frequent words. Unfortunately, in most cases, wordclouds are not used either correctly or efficiently. (Let’s check Google for some examples). library(wordcloud) library(&quot;RColorBrewer&quot;) test_set_tidy_clean &lt;- test_set_tidy %&gt;% anti_join(stop_words, by=&quot;word&quot;) %&gt;% count(word, sort=T) set.seed(1234) wordcloud(words=test_set_tidy_clean$word, freq=test_set_tidy_clean$n, min.freq = 1, rot.per = .25, random.order=FALSE, #scale=c(5,.5), max.words=150, colors=brewer.pal(8, &quot;Dark2&quot;)) What can we glean out form this wordcloud? Create a wordcloud for obituaries. # your code; your response Create a wordcloud for obituaries, but without stop words. # your code; your response Create a wordcloud for obituaries, but on lemmatized texts and without stop words. # your code; your response Summarize your observations below. What does stand out in these different versions of wordclouds? Which of the wordclouds you find more efficient? Can you think of some scenarios when a different type of wordcloud can be more efficient? Why? you answer goes here For more details on generating word clouds in R, see: http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know. 7.5 Word Distribution Plots 7.5.1 Simple — a Star Wars Example This kind of plot works better with texts rather than with newspapers. Let’s take a look at a script of Episode I: SW_to_DF &lt;- function(path_to_file, episode){ sw_sentences &lt;- scan(path_to_file, what=&quot;character&quot;, sep=&quot;\\n&quot;) sw_sentences &lt;- as.character(sw_sentences) sw_sentences &lt;- gsub(&quot;([A-Z]) ([A-Z])&quot;, &quot;\\\\1_\\\\2&quot;, sw_sentences) sw_sentences &lt;- gsub(&quot;([A-Z])-([A-Z])&quot;, &quot;\\\\1_\\\\2&quot;, sw_sentences) sw_sentences &lt;- as.data.frame(cbind(episode, sw_sentences), stringsAsFactors=FALSE) colnames(sw_sentences) &lt;- c(&quot;episode&quot;, &quot;sentences&quot;) return(sw_sentences) } sw1_df &lt;- SW_to_DF(paste0(pathToFiles, &quot;sw1.md&quot;), &quot;sw1&quot;) sw1_df_tidy &lt;- sw1_df %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(sentences, regex(&quot;^#&quot;, ignore_case = TRUE)))) sw1_df_tidy &lt;- sw1_df_tidy %&gt;% unnest_tokens(word, sentences) Try names of different characters (shmi, padme, anakin, sebulba, yoda, sith), or other terms that you know are tied to a specific part of the movie (pod, naboo, gungans, coruscant). ourWord = &quot;yoda&quot; word_occurance_vector &lt;- which(sw1_df_tidy$word == ourWord) plot(0, type=&#39;n&#39;, #ann=FALSE, xlim=c(1,length(sw1_df_tidy$word)), ylim=c(0,1), main=paste0(&quot;Dispersion Plot of `&quot;, ourWord, &quot;` in SW1&quot;), xlab=&quot;Movie Time&quot;, ylab=ourWord, yaxt=&quot;n&quot;) segments(x0=word_occurance_vector, x1=word_occurance_vector, y0=0, y1=2) 7.6 Word Distribution Plots: With Frequencies Over Time For newspapers—and other diachronic corpora—a different approach will work better: d1862 &lt;- read.delim(paste0(pathToFiles, &quot;dispatch_1862.tsv&quot;), encoding=&quot;UTF-8&quot;, header=TRUE, quote=&quot;&quot;, stringsAsFactors = FALSE) test_set &lt;- d1862 test_set$date &lt;- as.Date(test_set$date, format=&quot;%Y-%m-%d&quot;) test_set_tidy &lt;- test_set %&gt;% mutate(item_number = cumsum(str_detect(text, regex(&quot;^&quot;, ignore_case = TRUE)))) %&gt;% select(-type) %&gt;% unnest_tokens(word, text) %&gt;% mutate(word_number = row_number()) head(test_set_tidy) ## id date header item_number word word_number ## 1 1862-06-25_article_1 1862-06-25 The lines. 1 the 1 ## 2 1862-06-25_article_1 1862-06-25 The lines. 1 lines 2 ## 3 1862-06-25_article_1 1862-06-25 The lines. 1 on 3 ## 4 1862-06-25_article_1 1862-06-25 The lines. 1 monday 4 ## 5 1862-06-25_article_1 1862-06-25 The lines. 1 night 5 ## 6 1862-06-25_article_1 1862-06-25 The lines. 1 signal 6 Now, we can calculate frequencies of all words by dates: test_set_tidy_freqDay &lt;- test_set_tidy %&gt;% anti_join(stop_words, by=&quot;word&quot;) %&gt;% group_by(date) %&gt;% count(word) head(test_set_tidy_freqDay) ## # A tibble: 6 × 3 ## # Groups: date [1] ## date word n ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; ## 1 1862-01-01 000 13 ## 2 1862-01-01 007 1 ## 3 1862-01-01 014 1 ## 4 1862-01-01 1 10 ## 5 1862-01-01 10 5 ## 6 1862-01-01 100 2 We now can build a graph of word occurences over time. In the example below we search for manassas, which is the place where the the Second Battle of Bull Run (or, the Second Battle of Manassas) took place on August 28-30, 1862. The battle ended in Confederate victory. Our graph shows the spike of mentions of Manassas in the first days of September — right after the battle took place. Such graphs can be used to monitor discussions of different topic in chronological perspective. # interesting examples: # deserters, killed, # donelson (The Battle of Fort Donelson took place in early February of 1862), # manassas (place of the Second Bull Run, fought in August 28–30, 1862), # shiloh (Battle of Shiloh took place in April of 1862) ourWord = &quot;manassas&quot; test_set_tidy_word &lt;- test_set_tidy_freqDay %&gt;% filter(word==ourWord) plot(x=test_set_tidy_word$date, y=test_set_tidy_word$n, type=&quot;l&quot;, lty=3, lwd=1, main=paste0(&quot;Word `&quot;, ourWord, &quot;` over time&quot;), xlab = &quot;1862 - Dispatch coverage&quot;, ylab = &quot;word frequency per day&quot;) segments(x0=test_set_tidy_word$date, x1=test_set_tidy_word$date, y0=0, y1=test_set_tidy_word$n, lty=1, lwd=2) The graph like this can be used in a different way. Try words killed and deserters. When do these words spike? Can you interpret these graphs? your response goes here 7.7 KWIC: Keywords-in-Context Keywords-in-context is the most common method for creating concordances — a view that that allows us to go through all instances of specific words or word forms in order to understand how they are used. The quanteda library offers a very quick and easy application of this method: library(quanteda) library(readtext) dispatch1862 &lt;- readtext(paste0(pathToFiles, &quot;dispatch_1862.tsv&quot;), text_field = &quot;text&quot;, quote=&quot;&quot;) dispatch1862corpus &lt;- corpus(dispatch1862) Now, we can query the created corpus object using this command: kwic(YourCorpusObject, pattern = YourSearchPattern). pattern= can also take vectors (for example, c(\"soldier*\", \"troop*\")); you can also search for phrases with pattern=phrase(\"fort donelson\"); window= defines how many words will be shown before and after the match. kwic_test &lt;- kwic(dispatch1862corpus, pattern = &#39;lincoln&#39;, window=5) ## Warning: &#39;kwic.corpus()&#39; is deprecated. Use &#39;tokens()&#39; first. head(kwic_test) ## Keyword-in-context with 6 matches. ## [dispatch_1862.tsv.57, 886] their annihilation which the infamous | Lincoln ## [dispatch_1862.tsv.58, 8] . - The Premier of | Lincoln ## [dispatch_1862.tsv.60, 1015] Saturday, two of the | Lincoln ## [dispatch_1862.tsv.129, 1447] Old Abe and Mrs. | Lincoln ## [dispatch_1862.tsv.129, 1474] kangaroo-looking person. Mrs. | Lincoln ## [dispatch_1862.tsv.129, 1530] , and wire-pullers of the | Lincoln ## ## | Government is furnishing every day ## | has declared over and over ## | gunboats came down within sight ## | , several times. Old ## | was out in her carriage ## | dynasty are violent, abusive To view results better, we can remove unnecessary columns: kwic_test %&gt;% as_tibble %&gt;% select(pre, keyword, post) %&gt;% head(15) ## # A tibble: 15 × 3 ## pre keyword post ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 their annihilation which the infamous Lincoln Government is furnishing every… ## 2 . - The Premier of Lincoln has declared over and over ## 3 Saturday , two of the Lincoln gunboats came down within sight ## 4 Old Abe and Mrs . Lincoln , several times . Old ## 5 kangaroo-looking person . Mrs . Lincoln was out in her carriage ## 6 , and wire-pullers of the Lincoln dynasty are violent , abusive ## 7 against the boasted legions of Lincoln . ## 8 , and that Mr . Lincoln will make Gen . Banks ## 9 in the employment of the Lincoln Government had come in from ## 10 pulpit by the hirelings of Lincoln for declining to pray for ## 11 the oath of allegiance to Lincoln . It must be stipulated ## 12 off since that time . Lincoln , it will be recollected ## 13 , unexpected visit of President Lincoln , who arrived at the ## 14 the camps that Mr . Lincoln had come to have a ## 15 vociferous greeting . Mr . Lincoln rode at the right of NB: quanteda is quite a robust library. Check this page with examples for other possible quick experiments: https://quanteda.io/articles/pkgdown/examples/plotting.html 7.8 Homework Read about ngrams in Chapter 4. Relationships between words: n-grams and correlations (https://www.tidytextmining.com/ngrams.html), in https://www.tidytextmining.com/. Using what you have learned in this chapter identify and analyze bigrams in Dispatch, 1862. Submit the results of your analysis as an R notebook, as usual. You are welcome to work in groups. Optional: Work through Chapter 9 of Arnold, Taylor, and Lauren Tilton. 2015. Humanities Data in R. New York, NY: Springer Science+Business Media. (on Moodle!): create a notebook with all the code discusses there and send it via email (share via DropBox or some other service, if too large). DataCamp Assignments. 7.9 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["text-analysis-ii-distances-keywords-summarization.html", "8 Text Analysis II: Distances, Keywords, Summarization 8.1 Goals 8.2 Preliminaries 8.3 Document similarity/distance measures: text2vec library 8.4 TF-IDF 8.5 Text summarization 8.6 Homework 8.7 Submitting homework", " 8 Text Analysis II: Distances, Keywords, Summarization 8.1 Goals similarity distances; keyword extraction (tf-idf); text summarization techniques; 8.2 Preliminaries 8.2.1 Data prep_fun = function(x) { x %&gt;% str_to_lower %&gt;% # make text lower case str_replace_all(&quot;[^[:alnum:]]&quot;, &quot; &quot;) %&gt;% # remove non-alphanumeric symbols str_replace_all(&quot;\\\\s+&quot;, &quot; &quot;) # collapse multiple spaces } d1862 &lt;- read.delim(&quot;./files/data/dispatch_1862.tsv&quot;, encoding=&quot;UTF-8&quot;, header=TRUE, quote=&quot;&quot;) The following are the libraries that we will need for this section. Install those that you do not have yet. #install.packages(&quot;tidyverse&quot;, &quot;readr&quot;, &quot;stringr&quot;, &quot;text2vec&quot;) #install.packages(&quot;tidytext&quot;, &quot;wordcloud&quot;, &quot;RColorBrewer&quot;&quot;, &quot;quanteda&quot;, &quot;readtext&quot;, &quot;igraph&quot;) # General ones library(tidyverse) library(readr) library(&quot;RColorBrewer&quot;) # text analysis specific library(stringr) library(text2vec) library(tidytext) library(wordcloud) library(quanteda) library(readtext) library(igraph) 8.3 Document similarity/distance measures: text2vec library Document similarity—or distance—measures are valuable for a variety of tasks, such as identification of texts with similar (or the same) content. Let’s just filter it down to some sample that would not take too much time to process. We also need to clean up our texts for better calculations. 8.3.1 Distance Measures: Jaccard index, Cosine similarity, Euclidean distance The text2vec library can calculate a several different kinds of distances (details: http://text2vec.org/similarity.html): Jaccard, cosine, and Euclidean. 8.3.1.1 Jaccard similarity/index is a simple measure of similarity based on the comparison of two sets, namely, as the proportion of the number of common words to the number of unique words in both documents. Jaccard similarity takes only unique set of words for each sentence/document (https://en.wikipedia.org/wiki/Jaccard_index). Jaccard index is commonly used to find text that deal with the same subjects (share same vocabulary — frequencies of words have no effect on this measure) Jaccard similarity measures the similarity between two nominal attributes by taking the intersection of both and divide it by their union. 8.3.1.2 Cosine similarity another approach that measures similarity based on the content overlap between documents: each document is represented as a bag-of-words and as a sparse vector; the measure of overlap is defined as angle between vectors. Cosine similarity is better when we compare texts of varied length (angle of vectors, instead of distance). (https://en.wikipedia.org/wiki/Cosine_similarity) Cosine similarity measures the similarity between two vectors by taking the cosine of the angle the two vectors make in their dot product space. If the angle is zero, their similarity is one, the larger the angle is, the smaller their similarity. The measure is independent of vector length. 8.3.1.3 Euclidean distance one of the most common measures—a straight-line distance between two points in Euclidian space; based on word frequencies and most commonly used to find duplicates (https://en.wikipedia.org/wiki/Euclidean_distance). NB: more detailed explanations, see https://cmry.github.io/notes/euclidean-v-cosine 8.3.1.4 Testing… Let’s try a small and simple example first. sentences = c(&quot;The Caliph arrived to Baghdad from Mecca.&quot;, &quot;The Caliph arrived to Mecca from Baghdad.&quot;, &quot;The Caliph arrived from Mecca to Baghdad. The Caliph arrived from Baghdad to Mecca.&quot;, &quot;The Caliph arrived to Baghdad from Mecca. The Caliph arrived. The Caliph arrived. The Caliph arrived.&quot;, &quot;The Caliph arrived to Baghdad from Mecca. The Caliph returned to Mecca from Baghdad.&quot;, &quot;The Caliph arrived from Mecca to Baghdad, and then returned to Mecca.&quot;, &quot;The vezier arrived from Isbahan to Mecca. The Caliph, Caliph, Caliph returned from Mecca to Baghdad Baghdad Baghdad.&quot;) testDF &lt;- data.frame(&quot;ID&quot; = as.character(1:length(sentences)), &quot;TEXT&quot; = sentences) testDF$TEXT &lt;- prep_fun(testDF$TEXT) Now, converting to text2vec format: # shared vector space it = itoken(as.vector(testDF$TEXT)) v = create_vocabulary(it) vectorizer = vocab_vectorizer(v) # creating matrices sparseMatrix = create_dtm(it, vectorizer) denseMatrix = as.matrix(sparseMatrix) Let’s take a look inside: denseMatrix ## and isbahan then vezier returned from arrived baghdad mecca to the caliph ## 1 0 0 0 0 0 1 1 1 1 1 1 1 ## 2 0 0 0 0 0 1 1 1 1 1 1 1 ## 3 0 0 0 0 0 2 2 2 2 2 2 2 ## 4 0 0 0 0 0 1 4 1 1 1 4 4 ## 5 0 0 0 0 1 2 1 2 2 2 2 2 ## 6 1 0 1 0 1 1 1 1 2 2 1 1 ## 7 0 1 0 1 1 2 1 3 2 2 2 3 sparseMatrix ## 7 x 12 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 12 column names &#39;and&#39;, &#39;isbahan&#39;, &#39;then&#39; ... ]] ## ## 1 . . . . . 1 1 1 1 1 1 1 ## 2 . . . . . 1 1 1 1 1 1 1 ## 3 . . . . . 2 2 2 2 2 2 2 ## 4 . . . . . 1 4 1 1 1 4 4 ## 5 . . . . 1 2 1 2 2 2 2 2 ## 6 1 . 1 . 1 1 1 1 2 2 1 1 ## 7 . 1 . 1 1 2 1 3 2 2 2 3 Let’s generate our distance matrices: jaccardMatrix = sim2(sparseMatrix, method = &quot;jaccard&quot;, norm = &quot;none&quot;) cosineMatrix = sim2(sparseMatrix, method = &quot;cosine&quot;, norm = &quot;l2&quot;) euclideanMatrix = dist2(denseMatrix, method = &quot;euclidean&quot;, norm=&quot;l2&quot;) NB: Now, let’s check against the actual sentences: testDF$TEXT ## [1] &quot;the caliph arrived to baghdad from mecca &quot; ## [2] &quot;the caliph arrived to mecca from baghdad &quot; ## [3] &quot;the caliph arrived from mecca to baghdad the caliph arrived from baghdad to mecca &quot; ## [4] &quot;the caliph arrived to baghdad from mecca the caliph arrived the caliph arrived the caliph arrived &quot; ## [5] &quot;the caliph arrived to baghdad from mecca the caliph returned to mecca from baghdad &quot; ## [6] &quot;the caliph arrived from mecca to baghdad and then returned to mecca &quot; ## [7] &quot;the vezier arrived from isbahan to mecca the caliph caliph caliph returned from mecca to baghdad baghdad baghdad &quot; For convenience, here they are again, in a more readable form: The Caliph arrived to Baghdad from Mecca. The Caliph arrived to Mecca from Baghdad. The Caliph arrived from Mecca to Baghdad. The Caliph arrived from Baghdad to Mecca. The Caliph arrived to Baghdad from Mecca. The Caliph arrived. The Caliph arrived. The Caliph arrived. The Caliph arrived to Baghdad from Mecca. The Caliph returned to Mecca from Baghdad. The Caliph arrived from Mecca to Baghdad, and then returned to Mecca. The Vezier arrived from Isbahan to Mecca. The Caliph, Caliph, Caliph returned from Mecca to Baghdad Baghdad Baghdad. print(&quot;JACCARD: 1 is full match&quot;); jaccardMatrix ## [1] &quot;JACCARD: 1 is full match&quot; ## 7 x 7 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 2 3 4 5 6 7 ## 1 1.000 1.000 1.000 1.000 0.875 0.7000000 0.7000000 ## 2 1.000 1.000 1.000 1.000 0.875 0.7000000 0.7000000 ## 3 1.000 1.000 1.000 1.000 0.875 0.7000000 0.7000000 ## 4 1.000 1.000 1.000 1.000 0.875 0.7000000 0.7000000 ## 5 0.875 0.875 0.875 0.875 1.000 0.8000000 0.8000000 ## 6 0.700 0.700 0.700 0.700 0.800 1.0000000 0.6666667 ## 7 0.700 0.700 0.700 0.700 0.800 0.6666667 1.0000000 print(&quot;COSINE: 1 is full match&quot;); cosineMatrix ## [1] &quot;COSINE: 1 is full match&quot; ## 7 x 7 sparse Matrix of class &quot;dsCMatrix&quot; ## 1 2 3 4 5 6 7 ## 1 1.0000000 1.0000000 1.0000000 0.8386279 0.9636241 0.8504201 0.9197090 ## 2 1.0000000 1.0000000 1.0000000 0.8386279 0.9636241 0.8504201 0.9197090 ## 3 1.0000000 1.0000000 1.0000000 0.8386279 0.9636241 0.8504201 0.9197090 ## 4 0.8386279 0.8386279 0.8386279 1.0000000 0.7614996 0.6240377 0.7423701 ## 5 0.9636241 0.9636241 0.9636241 0.7614996 1.0000000 0.8825226 0.9544271 ## 6 0.8504201 0.8504201 0.8504201 0.6240377 0.8825226 1.0000000 0.8111071 ## 7 0.9197090 0.9197090 0.9197090 0.7423701 0.9544271 0.8111071 1.0000000 print(&quot;EUCLIDEAN: 0 is full match&quot;); euclideanMatrix ## [1] &quot;EUCLIDEAN: 0 is full match&quot; ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.0000000 0.0000000 0.0000000 0.5681059 0.2697254 0.5469551 0.4007268 ## [2,] 0.0000000 0.0000000 0.0000000 0.5681059 0.2697254 0.5469551 0.4007268 ## [3,] 0.0000000 0.0000000 0.0000000 0.5681059 0.2697254 0.5469551 0.4007268 ## [4,] 0.5681059 0.5681059 0.5681059 0.0000000 0.6906524 0.8671358 0.7178160 ## [5,] 0.2697254 0.2697254 0.2697254 0.6906524 0.0000000 0.4847213 0.3019035 ## [6,] 0.5469551 0.5469551 0.5469551 0.8671358 0.4847213 0.0000000 0.6146428 ## [7,] 0.4007268 0.4007268 0.4007268 0.7178160 0.3019035 0.6146428 0.0000000 All three distances tell us that 1, 2, and 3 are the “same”. But when it comes to 4, the situation changes: Jaccard is most efficient, then Cosine, and Euclidean is least useful. If we want to find both 1 and 7, Cosine is the most effective, and Euclidean is the least effective. Perhaps: Jaccard &gt; overlap; Cosine &gt; similarity; Euclidean &gt; exactness? Additional read: https://cmry.github.io/notes/euclidean-v-cosine (although python is used here) 8.3.2 Now, let’s run this on “Dispatch” sample_d1862 &lt;- d1862 %&gt;% filter(type==&quot;advert&quot;) sample_d1862$text &lt;- prep_fun(sample_d1862$text) # shared vector space it = itoken(as.vector(sample_d1862$text)) v = create_vocabulary(it) %&gt;% prune_vocabulary(term_count_min = 3) # vectorizer = vocab_vectorizer(v) prune_vocabulary() is a useful function if you work with a large corpus; using term_count_min= would allow to remove low frequency vocabulary from our vector space and lighten up calculations. Now, we need to create a document-feature matrix: dtmD = create_dtm(it, vectorizer) jaccardMatrix = sim2(dtmD, dtmD, method = &quot;jaccard&quot;, norm = &quot;none&quot;) jaccardMatrix@Dimnames[[1]] &lt;- as.vector(sample_d1862$id) jaccardMatrix@Dimnames[[2]] &lt;- as.vector(sample_d1862$id) Let’s take a look at a small section of our matrix. Can you read it? How should this data look in tidy format? jaccardMatrix[1:4, 1:2] ## 4 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## 1862-01-10_advert_108 1862-01-10_advert_109 ## 1862-01-10_advert_108 1.00000000 0.07476636 ## 1862-01-10_advert_109 0.07476636 1.00000000 ## 1862-01-11_advert_230 0.04109589 0.09016393 ## 1862-01-11_advert_231 0.04912281 0.08474576 Converting matrix into a proper tidy data frame is a bit tricky. Luckily, igraph library can be extremely helpful here. We can treat our matrix as edges, where each number is the weight of each given edge. Loading this data into igraph will help us to avoid heavy-lifting on conversion as it can do all the complicated reconfiguration of our data, converting it into a proper dataframe that conforms to the principles of tidy data. All steps include: convert our initial object from a sparse matrix format into a regular matrix format; rename rows and columns (we have done this already though); create igraph object from our regular matrix; extract edges dataframe. jaccardMatrix &lt;- as.matrix(jaccardMatrix) library(igraph) jaccardNW &lt;- graph.adjacency(jaccardMatrix, mode=&quot;undirected&quot;, weighted=TRUE) jaccardNW &lt;- simplify(jaccardNW) jaccard_sim_df &lt;- as_data_frame(jaccardNW, what=&quot;edges&quot;) colnames(jaccard_sim_df) &lt;- c(&quot;text1&quot;, &quot;text2&quot;, &quot;jaccardSimilarity&quot;) jaccard_sim_df &lt;- jaccard_sim_df %&gt;% arrange(desc(jaccardSimilarity)) head(jaccard_sim_df, 10) ## text1 text2 jaccardSimilarity ## 1 1862-01-11_advert_231 1862-01-13_advert_273 1 ## 2 1862-09-08_advert_26 1862-10-13_advert_293 1 ## 3 1862-04-22_advert_244 1862-04-05_advert_248 1 ## 4 1862-04-07_advert_127 1862-04-05_advert_235 1 ## 5 1862-04-07_advert_162 1862-04-05_advert_183 1 ## 6 1862-04-07_advert_189 1862-04-05_advert_161 1 ## 7 1862-04-07_advert_250 1862-04-05_advert_255 1 ## 8 1862-04-07_advert_253 1862-04-05_advert_289 1 ## 9 1862-04-07_advert_262 1862-04-05_advert_250 1 ## 10 1862-04-07_advert_263 1862-04-05_advert_251 1 t_jaccard_sim_df_subset &lt;- jaccard_sim_df %&gt;% filter(jaccardSimilarity &gt; 0.49) %&gt;% filter(jaccardSimilarity &lt;= 0.9) %&gt;% arrange(desc(jaccardSimilarity), .by_group=T) head(t_jaccard_sim_df_subset, 10) ## text1 text2 jaccardSimilarity ## 1 1862-02-10_advert_149 1862-04-05_advert_303 0.9000000 ## 2 1862-04-07_advert_30 1862-04-05_advert_237 0.9000000 ## 3 1862-04-07_advert_175 1862-04-05_advert_38 0.9000000 ## 4 1862-01-11_advert_230 1862-01-13_advert_272 0.8979592 ## 5 1862-06-09_advert_63 1862-05-12_advert_219 0.8974359 ## 6 1862-03-10_advert_146 1862-04-05_advert_261 0.8928571 ## 7 1862-06-09_advert_309 1862-05-12_advert_233 0.8913043 ## 8 1862-02-10_advert_224 1862-03-10_advert_13 0.8913043 ## 9 1862-06-09_advert_331 1862-04-07_advert_165 0.8888889 ## 10 1862-06-09_advert_339 1862-04-07_advert_250 0.8888889 Let’s check the texts of 1862-04-07_advert_175 and 1862-04-05_advert_38, which have the score of 0.9000000 (a close match). example &lt;- d1862 %&gt;% filter(id==&quot;1862-04-07_advert_175&quot;) [1] &quot;Very desirable Residence on the South of Main st., between and Cheery streets, in Sidney, at Auction. -- We will sell, upon the premises, on Monday, the 7th day of April, at 4½ o&#39;clock P. M., a very comfortable and well arranged Framed Residence located as above, and now in the occupancy of Mr. Wm. B Davidson It 7 rooms with closed, kitchen and all accessary out building, and is particularly adapted for the accommodation of a medium family. The location of this house is as desirable as any in Sidney; is located in a very pleasant neighborhood, within a few minutes walk of the business portion of the city. The lot fronts 30 feet and runs back 189 feet to an alley 30 feet wide. Terms. -- One-third cash; the balance at 6 and 12 months, for negotiable notes, with interest added, and secured by a trust deed. The purchaser to pay the taxes and insurance for 1862. Jas. M. Taylor, &amp; Son, Auctioneers. mh 27&quot; example &lt;- d1862 %&gt;% filter(id==&quot;1862-04-05_advert_38&quot;) [1] &quot;Very desirable Framed Residence of the South side of Main St. Between culvert and Cherri streets. In Sidney, at Auction. -- We will sell, upon the premises, on Monday, the 7th day of April, at 4½ o&#39;clock P. M. a very comfortable and well arranged Framed. Residence located as above, and now in the occupancy of Mr. Wm. B Davidson. It contains 7 rooms, with closets, kitchen and all necessary out buildings, and is particularly adapted for the accommodation of a medium sized family. The location of this house is as desirable as any in Sidney; is located in a very pleasant neighborhood, and within a few minutes walk of the business portion of the city. The lot fronts 80 feet and runs back 189 feet to an alley 20 feet wide. Terms. -- One-third cash, the balance at 6 and 12 months, for negotiable notes, with interest added, and secured by a trust deed. The purchaser to pay the taxes and insurance for 1862 Jas. M. Taylor &amp; Son. mh 27 Auctioneers.&quot; Check http://text2vec.org/similarity.html and calculate cosine and euclidean distances for the same set of texts. What is the score for the same two texts? How do these scores differ in your opinion? your observations Choose one of the distance measures and take a close look at a subset of texts with the closest match (i.e. find a text which has the highest number of complete matches — 1.0). Try to apply as many techniques as possible in your analysis (e.g., frequency lists, wordclouds, graphing over time, etc.) your analysis, your code… 8.4 TF-IDF Before we proceed, let’s load some text. Below is an example of how you can load a text using its URL. However, be mindful about using this approach: it is convenient with a small number of short texts, but not efficient with large number of long texts. urlUDHR &lt;- &quot;https://univie-histr-2020s.github.io/files/UDHR.csv&quot; udhr &lt;- read.delim(url(urlUDHR), encoding=&quot;UTF-8&quot;, header=TRUE, quote=&quot;&quot;, stringsAsFactors = FALSE) udhrTidy &lt;- udhr %&gt;% unnest_tokens(WORD, TEXT) %&gt;% count(SECTION, WORD, sort=TRUE) summary(udhrTidy) ## SECTION WORD n ## Length:1134 Length:1134 Min. : 1.000 ## Class :character Class :character 1st Qu.: 1.000 ## Mode :character Mode :character Median : 1.000 ## Mean : 1.515 ## 3rd Qu.: 1.000 ## Max. :26.000 From Wikipedia: In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document’s relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. udhr_TFIDF &lt;- udhrTidy %&gt;% bind_tf_idf(WORD, SECTION, n) %&gt;% arrange(desc(tf_idf)) %&gt;% ungroup udhr_TFIDF %&gt;% filter(tf_idf &gt;= 0.15) ## SECTION WORD n tf idf tf_idf ## 1 Article 4 slavery 2 0.09523810 3.433987 0.3270464 ## 2 Article 15 nationality 3 0.11538462 2.740840 0.3162508 ## 3 Article 3 liberty 1 0.09090909 3.433987 0.3121807 ## 4 Article 9 arrest 1 0.09090909 3.433987 0.3121807 ## 5 Article 9 detention 1 0.09090909 3.433987 0.3121807 ## 6 Article 9 exile 1 0.09090909 3.433987 0.3121807 ## 7 Article 6 everywhere 1 0.07692308 3.433987 0.2641529 ## 8 Article 9 arbitrary 1 0.09090909 2.740840 0.2491673 ## 9 Article 20 association 2 0.08695652 2.740840 0.2383339 ## 10 Article 5 cruel 1 0.06250000 3.433987 0.2146242 ## 11 Article 5 degrading 1 0.06250000 3.433987 0.2146242 ## 12 Article 5 inhuman 1 0.06250000 3.433987 0.2146242 ## 13 Article 5 punishment 1 0.06250000 3.433987 0.2146242 ## 14 Article 5 torture 1 0.06250000 3.433987 0.2146242 ## 15 Article 5 treatment 1 0.06250000 3.433987 0.2146242 ## 16 Article 3 life 1 0.09090909 2.335375 0.2123068 ## 17 Article 3 security 1 0.09090909 2.335375 0.2123068 ## 18 Article 9 subjected 1 0.09090909 2.335375 0.2123068 ## 19 Article 7 discrimination 3 0.07692308 2.740840 0.2108338 ## 20 Article 17 property 2 0.07692308 2.740840 0.2108338 ## 21 Article 6 before 1 0.07692308 2.740840 0.2108338 ## 22 Article 12 attacks 2 0.05263158 3.433987 0.1807362 ## 23 Article 24 holidays 1 0.05263158 3.433987 0.1807362 ## 24 Article 24 hours 1 0.05263158 3.433987 0.1807362 ## 25 Article 24 leisure 1 0.05263158 3.433987 0.1807362 ## 26 Article 24 reasonable 1 0.05263158 3.433987 0.1807362 ## 27 Article 24 rest 1 0.05263158 3.433987 0.1807362 ## 28 Article 24 working 1 0.05263158 3.433987 0.1807362 ## 29 Article 6 recognition 1 0.07692308 2.335375 0.1796442 ## 30 Article 3 person 1 0.09090909 1.824549 0.1658681 ## 31 Article 8 by 3 0.11111111 1.488077 0.1653419 ## 32 Article 4 forms 1 0.04761905 3.433987 0.1635232 ## 33 Article 4 prohibited 1 0.04761905 3.433987 0.1635232 ## 34 Article 4 servitude 1 0.04761905 3.433987 0.1635232 ## 35 Article 4 slave 1 0.04761905 3.433987 0.1635232 ## 36 Article 26 education 7 0.05882353 2.740840 0.1612259 hist(udhr_TFIDF$tf_idf) Let’s take a look at any of the Articles: articleID = &quot;Article 4&quot; temp &lt;- filter(udhr_TFIDF, SECTION==articleID) %&gt;% arrange(desc(tf_idf)) temp ## SECTION WORD n tf idf tf_idf ## 1 Article 4 slavery 2 0.09523810 3.4339872 0.327046400 ## 2 Article 4 forms 1 0.04761905 3.4339872 0.163523200 ## 3 Article 4 prohibited 1 0.04761905 3.4339872 0.163523200 ## 4 Article 4 servitude 1 0.04761905 3.4339872 0.163523200 ## 5 Article 4 slave 1 0.04761905 3.4339872 0.163523200 ## 6 Article 4 trade 1 0.04761905 2.7408400 0.130516192 ## 7 Article 4 held 1 0.04761905 2.3353749 0.111208329 ## 8 Article 4 their 1 0.04761905 2.3353749 0.111208329 ## 9 Article 4 shall 2 0.09523810 0.7949299 0.075707607 ## 10 Article 4 all 1 0.04761905 1.3545457 0.064502174 ## 11 Article 4 one 1 0.04761905 1.2367626 0.058893458 ## 12 Article 4 be 2 0.09523810 0.6007739 0.057216558 ## 13 Article 4 no 1 0.04761905 1.1314021 0.053876291 ## 14 Article 4 in 2 0.09523810 0.5436154 0.051772900 ## 15 Article 4 or 1 0.04761905 0.7259370 0.034568429 ## 16 Article 4 and 1 0.04761905 0.2559334 0.012187304 ## 17 Article 4 the 1 0.04761905 0.1017827 0.004846795 articleID = &quot;Article 26&quot; temp &lt;- filter(udhr_TFIDF, SECTION==articleID) %&gt;% arrange(desc(tf_idf)) temp ## SECTION WORD n tf idf tf_idf ## 1 Article 26 education 7 0.058823529 2.74084002 0.161225884 ## 2 Article 26 elementary 2 0.016806723 3.43398720 0.057714071 ## 3 Article 26 shall 8 0.067226891 0.79492987 0.053440664 ## 4 Article 26 fundamental 2 0.016806723 2.04769284 0.034415006 ## 5 Article 26 human 2 0.016806723 2.04769284 0.034415006 ## 6 Article 26 nations 2 0.016806723 2.04769284 0.034415006 ## 7 Article 26 be 6 0.050420168 0.60077386 0.030291119 ## 8 Article 26 accessible 1 0.008403361 3.43398720 0.028857035 ## 9 Article 26 activities 1 0.008403361 3.43398720 0.028857035 ## 10 Article 26 available 1 0.008403361 3.43398720 0.028857035 ## 11 Article 26 choose 1 0.008403361 3.43398720 0.028857035 ## 12 Article 26 compulsory 1 0.008403361 3.43398720 0.028857035 ## 13 Article 26 directed 1 0.008403361 3.43398720 0.028857035 ## 14 Article 26 equally 1 0.008403361 3.43398720 0.028857035 ## 15 Article 26 friendship 1 0.008403361 3.43398720 0.028857035 ## 16 Article 26 further 1 0.008403361 3.43398720 0.028857035 ## 17 Article 26 generally 1 0.008403361 3.43398720 0.028857035 ## 18 Article 26 given 1 0.008403361 3.43398720 0.028857035 ## 19 Article 26 groups 1 0.008403361 3.43398720 0.028857035 ## 20 Article 26 higher 1 0.008403361 3.43398720 0.028857035 ## 21 Article 26 least 1 0.008403361 3.43398720 0.028857035 ## 22 Article 26 maintenance 1 0.008403361 3.43398720 0.028857035 ## 23 Article 26 merit 1 0.008403361 3.43398720 0.028857035 ## 24 Article 26 parents 1 0.008403361 3.43398720 0.028857035 ## 25 Article 26 prior 1 0.008403361 3.43398720 0.028857035 ## 26 Article 26 professional 1 0.008403361 3.43398720 0.028857035 ## 27 Article 26 racial 1 0.008403361 3.43398720 0.028857035 ## 28 Article 26 religious 1 0.008403361 3.43398720 0.028857035 ## 29 Article 26 stages 1 0.008403361 3.43398720 0.028857035 ## 30 Article 26 strengthening 1 0.008403361 3.43398720 0.028857035 ## 31 Article 26 technical 1 0.008403361 3.43398720 0.028857035 ## 32 Article 26 tolerance 1 0.008403361 3.43398720 0.028857035 ## 33 Article 26 among 1 0.008403361 2.74084002 0.023032269 ## 34 Article 26 children 1 0.008403361 2.74084002 0.023032269 ## 35 Article 26 kind 1 0.008403361 2.74084002 0.023032269 ## 36 Article 26 made 1 0.008403361 2.74084002 0.023032269 ## 37 Article 26 peace 1 0.008403361 2.74084002 0.023032269 ## 38 Article 26 promote 1 0.008403361 2.74084002 0.023032269 ## 39 Article 26 understanding 1 0.008403361 2.74084002 0.023032269 ## 40 Article 26 all 2 0.016806723 1.35454566 0.022765473 ## 41 Article 26 for 2 0.016806723 1.23676263 0.020785927 ## 42 Article 26 basis 1 0.008403361 2.33537492 0.019624999 ## 43 Article 26 have 1 0.008403361 2.33537492 0.019624999 ## 44 Article 26 on 1 0.008403361 2.33537492 0.019624999 ## 45 Article 26 personality 1 0.008403361 2.33537492 0.019624999 ## 46 Article 26 respect 1 0.008403361 2.33537492 0.019624999 ## 47 Article 26 that 1 0.008403361 2.33537492 0.019624999 ## 48 Article 26 their 1 0.008403361 2.33537492 0.019624999 ## 49 Article 26 at 1 0.008403361 2.04769284 0.017207503 ## 50 Article 26 development 1 0.008403361 2.04769284 0.017207503 ## 51 Article 26 it 1 0.008403361 2.04769284 0.017207503 ## 52 Article 26 united 1 0.008403361 2.04769284 0.017207503 ## 53 Article 26 3 1 0.008403361 1.82454929 0.015332347 ## 54 Article 26 full 1 0.008403361 1.82454929 0.015332347 ## 55 Article 26 and 7 0.058823529 0.25593337 0.015054904 ## 56 Article 26 freedoms 1 0.008403361 1.64222774 0.013800233 ## 57 Article 26 free 1 0.008403361 1.48807706 0.012504849 ## 58 Article 26 of 6 0.050420168 0.21511138 0.010845952 ## 59 Article 26 rights 1 0.008403361 1.03609193 0.008706655 ## 60 Article 26 the 10 0.084033613 0.10178269 0.008553168 ## 61 Article 26 1 1 0.008403361 0.86903785 0.007302839 ## 62 Article 26 2 1 0.008403361 0.86903785 0.007302839 ## 63 Article 26 a 1 0.008403361 0.86903785 0.007302839 ## 64 Article 26 right 2 0.016806723 0.38946477 0.006545626 ## 65 Article 26 or 1 0.008403361 0.72593700 0.006100311 ## 66 Article 26 in 1 0.008403361 0.54361545 0.004568197 ## 67 Article 26 to 6 0.050420168 0.06669137 0.003362590 ## 68 Article 26 has 1 0.008403361 0.38946477 0.003272813 ## 69 Article 26 everyone 1 0.008403361 0.29849299 0.002508344 We can use wordcloud to vizualize results — but they will not be too telling, if we use word frequencies. library(wordcloud) library(&quot;RColorBrewer&quot;) set.seed(1234) wordcloud(words=temp$WORD, freq=temp$n, min.freq = 1, rot.per = .0, random.order=FALSE, scale=c(10,.5), max.words=150, colors=brewer.pal(8, &quot;Dark2&quot;)) Instead we can use tf_idf values: set.seed(1234) wordcloud(words=temp$WORD, freq=temp$tf_idf, min.freq = 1, rot.per = .0, random.order=FALSE, scale=c(10,.5), max.words=150, colors=brewer.pal(8, &quot;Dark2&quot;)) 8.4.1 Inaugural speeches of the US presidents The quanteda package includes a corpus of presidential inaugural speeches. What did the presidenst speak about? For thoughts and ideas, take a look at what Google News Lab did with this data: http://inauguratespeeches.com/. You can find readable addresses here: https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses. data(&quot;data_corpus_inaugural&quot;, package = &quot;quanteda&quot;) inaug_dfm &lt;- quanteda::dfm(data_corpus_inaugural, verbose = FALSE) ## Warning: &#39;dfm.corpus()&#39; is deprecated. Use &#39;tokens()&#39; first. head(inaug_dfm) ## Document-feature matrix of: 6 documents, 9,439 features (93.84% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_nfeat ... 9,429 more features ] The corpus is stored as a document-feature matrix, which we can convert into a more familiar tidy format in the following manner: inaug_td &lt;- tidy(inaug_dfm) head(inaug_td) ## # A tibble: 6 × 3 ## document term count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1789-Washington fellow-citizens 1 ## 2 1797-Adams fellow-citizens 3 ## 3 1801-Jefferson fellow-citizens 2 ## 4 1809-Madison fellow-citizens 1 ## 5 1813-Madison fellow-citizens 1 ## 6 1817-Monroe fellow-citizens 5 Using what you have learned so far, analyze speeches of American presidents: your code; you analysis; your observations… 8.5 Text summarization Before we proceed, let’s load some text. Below is an example of how you can load a text using its URL. However, be mindful about using this approach: it is convenient with a small number of short texts, but not efficient with large number of long texts. urlText &lt;- &quot;https://univie-histr-2020s.github.io/files/test_text.txt&quot; testText &lt;- scan(url(urlText), what=&quot;character&quot;, sep=&quot;\\n&quot;) We can use different algorithms to summarize texts, which in this context means extracting key sentences, whose keyness is calculated through different means. Library lexRankr used tf-idf values and some methods from social network analysis to identify the most central sentences in a text. (For technical details, see: https://cran.r-project.org/web/packages/lexRankr/lexRankr.pdf; for a detailed math description: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html). Take a look at the summary and then at the full text! library(lexRankr) textToSummarize = testText summary = lexRank(textToSummarize, docId = rep(1, length(textToSummarize)), #repeat same docid for all of input vector n = 5, # number of sentences continuous = TRUE) ## Parsing text into sentences and tokens...DONE ## Calculating pairwise sentence similarities...DONE ## Applying LexRank...DONE ## Formatting Output...DONE # this is just preparing results for better viewing summary$sentenceId &lt;- str_replace_all(summary$sentenceId, &quot;\\\\d+_&quot;, &quot;&quot;) summary$sentenceId &lt;- as.numeric(summary$sentenceId) summary &lt;- summary %&gt;% arrange(sentenceId) summary$sentence ## [1] &quot;The Klimt University of Vienna Ceiling Paintings, also known as the Faculty Paintings, were a series of paintings made by Gustav Klimt for the ceiling of the University of Vienna`s Great Hall between the years of 1900–1907.&quot; ## [2] &quot;Upon presenting his paintings, Philosophy, Medicine and Jurisprudence, Klimt came under attack for `pornography` and `perverted excess` in the paintings.&quot; ## [3] &quot;Klimt described the painting as follows: `On the left a group of figures, the beginning of life, fruition, decay.&quot; ## [4] &quot;In 1903, Hermann Bahr, a writer and a supporter of Klimt, in response to the criticism of the Faculty Paintings compiled articles which attacked Klimt, and published a book Gegen Klimt (Against Klimt) with his foreword, where he argued that the reactions were absurd.&quot; ## [5] &quot;In 1911 Medicine and Jurisprudence were bought by Klimt`s friend and fellow artist, Koloman Moser.[clarification needed] Medicine eventually came into the possession of a Jewish family, and in 1938 the painting was seized by Germany.&quot; [1] &quot;The Klimt University of Vienna Ceiling Paintings, also known as the Faculty Paintings, were a series of paintings made by Gustav Klimt for the ceiling of the University of Vienna`s Great Hall between the years of 1900–1907.&quot; [2] &quot;Upon presenting his paintings, Philosophy, Medicine and Jurisprudence, Klimt came under attack for `pornography` and `perverted excess` in the paintings.&quot; [3] &quot;Klimt described the painting as follows: `On the left a group of figures, the beginning of life, fruition, decay.&quot; [4] &quot;In 1903, Hermann Bahr, a writer and a supporter of Klimt, in response to the criticism of the Faculty Paintings compiled articles which attacked Klimt, and published a book Gegen Klimt (Against Klimt) with his foreword, where he argued that the reactions were absurd.&quot; [5] &quot;In 1911 Medicine and Jurisprudence were bought by Klimt`s friend and fellow artist, Koloman Moser.[clarification needed] Medicine eventually came into the possession of a Jewish family, and in 1938 the painting was seized by Germany.&quot; Here is another corpus of articles to play with (from tm library): library(tm) data(&quot;acq&quot;) acqTidy &lt;- tidy(acq) Here is an article: item = 1 test &lt;- str_replace_all(acqTidy$text[item], &quot;\\\\s+&quot;, &quot; &quot;) test ## [1] &quot;Computer Terminal Systems Inc said it has completed the sale of 200,000 shares of its common stock, and warrants to acquire an additional one mln shares, to &lt;Sedio N.V.&gt; of Lugano, Switzerland for 50,000 dlrs. The company said the warrants are exercisable for five years at a purchase price of .125 dlrs per share. Computer Terminal said Sedio also has the right to buy additional shares and increase its total holdings up to 40 pct of the Computer Terminal&#39;s outstanding common stock under certain circumstances involving change of control at the company. The company said if the conditions occur the warrants would be exercisable at a price equal to 75 pct of its common stock&#39;s market price at the time, not to exceed 1.50 dlrs per share. Computer Terminal also said it sold the technolgy rights to its Dot Matrix impact technology, including any future improvements, to &lt;Woodco Inc&gt; of Houston, Tex. for 200,000 dlrs. But, it said it would continue to be the exclusive worldwide licensee of the technology for Woodco. The company said the moves were part of its reorganization plan and would help pay current operation costs and ensure product delivery. Computer Terminal makes computer generated labels, forms, tags and ticket printers and terminals. Reuter&quot; [1] &quot;Computer Terminal Systems Inc said it has completed the sale of 200,000 shares of its common stock, and warrants to acquire an additional one mln shares, to &lt;Sedio N.V.&gt; of Lugano, Switzerland for 50,000 dlrs. The company said the warrants are exercisable for five years at a purchase price of .125 dlrs per share. Computer Terminal said Sedio also has the right to buy additional shares and increase its total holdings up to 40 pct of the Computer Terminal&#39;s outstanding common stock under certain circumstances involving change of control at the company. The company said if the conditions occur the warrants would be exercisable at a price equal to 75 pct of its common stock&#39;s market price at the time, not to exceed 1.50 dlrs per share. Computer Terminal also said it sold the technolgy rights to its Dot Matrix impact technology, including any future improvements, to &lt;Woodco Inc&gt; of Houston, Tex. for 200,000 dlrs. But, it said it would continue to be the exclusive worldwide licensee of the technology for Woodco. The company said the moves were part of its reorganization plan and would help pay current operation costs and ensure product delivery. Computer Terminal makes computer generated labels, forms, tags and ticket printers and terminals. Reuter&quot; Let’s see how our summary comes out: textToSummarize = test summary = lexRank(textToSummarize, docId = rep(1, length(textToSummarize)), #repeat same docid for all of input vector n = 3, # number of sentences continuous = TRUE) ## Parsing text into sentences and tokens...DONE ## Calculating pairwise sentence similarities...DONE ## Applying LexRank...DONE ## Formatting Output...DONE # this is just preparing results for better viewing summary$sentenceId &lt;- str_replace_all(summary$sentenceId, &quot;\\\\d+_&quot;, &quot;&quot;) summary$sentenceId &lt;- as.numeric(summary$sentenceId) summary &lt;- summary %&gt;% arrange(sentenceId) summary$sentence ## [1] &quot;Computer Terminal Systems Inc said it has completed the sale of 200,000 shares of its common stock, and warrants to acquire an additional one mln shares, to &lt;Sedio N.V.&gt; of Lugano, Switzerland for 50,000 dlrs.&quot; ## [2] &quot;Computer Terminal said Sedio also has the right to buy additional shares and increase its total holdings up to 40 pct of the Computer Terminal&#39;s outstanding common stock under certain circumstances involving change of control at the company.&quot; ## [3] &quot;The company said if the conditions occur the warrants would be exercisable at a price equal to 75 pct of its common stock&#39;s market price at the time, not to exceed 1.50 dlrs per share.&quot; [1] &quot;Computer Terminal Systems Inc said it has completed the sale of 200,000 shares of its common stock, and warrants to acquire an additional one mln shares, to &lt;Sedio N.V.&gt; of Lugano, Switzerland for 50,000 dlrs.&quot; [2] &quot;Computer Terminal said Sedio also has the right to buy additional shares and increase its total holdings up to 40 pct of the Computer Terminal&#39;s outstanding common stock under certain circumstances involving change of control at the company.&quot; [3] &quot;The company said if the conditions occur the warrants would be exercisable at a price equal to 75 pct of its common stock&#39;s market price at the time, not to exceed 1.50 dlrs per share.&quot; Try this with other articles — and compare summaries with full versions. Share your observations. your code; your observations 8.6 Homework given in the chapter. 8.7 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. "],["text-analysis-iii-finding-groups-of-texts.html", "9 Text Analysis III: Finding Groups of Texts 9.1 Goals 9.2 Preliminaries 9.3 Clustering 9.4 PCA viz for HCLUST 9.5 K-means clustering 9.6 Determining the optimal number of clusters: “Elbow Method” and “Average Silhouette Method” 9.7 Other “clustering” methods 9.8 Topic Modeling 9.9 Per-topic-per-word probabilities (beta) 9.10 Addendum: different distances code sample 9.11 Homework 9.12 Submitting homework 9.13 Additional Materials", " 9 Text Analysis III: Finding Groups of Texts 9.1 Goals introduce text clustering approaches: approaches that allow us find groups of thematically similar texts: hierarchical clustering; k-means clustering; topic modeling; 9.2 Preliminaries 9.2.1 Libraries The following are the libraries that we will need for this section. Install those that you do not have yet. #install.packages(&quot;ggplot2&quot;, &quot;LDAvis&quot;, &quot;readr&quot;, &quot;slam&quot;, &quot;stringr&quot;, &quot;tictoc&quot;, &quot;tidytext&quot;, &quot;tidyverse&quot;, &quot;tm&quot;, &quot;topicmodels&quot;) # general library(ggplot2) # text analysis specific library(readr) library(slam) library(stringr) library(tidytext) library(tidyverse) library(tm) library(topicmodels) library(text2vec) library(stylo) library(cluster) # clustering algorithms library(factoextra) # clustering algorithms &amp; visualization # extra library(tictoc) # to time operations 9.2.1.1 The Dispatch Data: Preprocessing The Richmond Dispatch (1861) The Richmond Dispatch (1862) The Richmond Dispatch (1863) The Richmond Dispatch (1864) Topic Models trained on 1864 Topic Models fir testing LDAvis We will limit to only one year, but, of course, the results are always more interesting with more data… Loading… d1864 &lt;- read.delim(&quot;./files/data/dispatch_1864.tsv&quot;, encoding=&quot;UTF-8&quot;, header=TRUE, quote=&quot;&quot;, stringsAsFactors = FALSE) d1864$date &lt;- as.Date(d1864$date, format=&quot;%Y-%m-%d&quot;) knitr::kable(head(d1864), table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) id date type header text 1864-04-28_article_1 1864-04-28 article The news. The news. Yesterday was an unusual dull day, there being a great dearth in the news market, which was the subject of general remark. – The report which prevailed towards night, to the effect that a battle was progressing on the line of the Rapidan, we feel authorized to contradict. All was quiet when the Central train left Gordonsville yesterday evening. The cannonading heard in the direction of Germanna Ford, previously noticed, was caused by artillery practice. 1864-04-28_article_2 1864-04-28 article Yankee rule in Plymouth. Yankee rule in Plymouth. The following orders are copies of hand-bills posted in the town of Plymouth. It will be seen that Brig. Gen. Wessels is a model after Lincoln’s own heart, and undertook to \" run the churches\" and the schools besides. As we find the names of the General and the Provost Marshal, and the A. A., G.’s on the register of the Libby Hotel, in this city, it is more than likely that the children \" between eight and fourteen\" in Plymouth are having a cheerful vacation, and that Col. Moffitt will refrain for the present from the disagreeable duty of reporting the derelict heads of families who don’t enforce their attendance. This is the school order: Notice. The inhabitants of Plymouth are hereby notified that a Free school, for white children, will be spend under competent teachers, On Monday, 18th inst, in the Episcopal Church. The attention of parents and guardians is called in this important subject; and it is expected that all children between eight and fourteen years of age will attend the school. Those over fourteen may attend if they wish. Lieut. Col. Moffitt, Provost Marshal, will institute careful inquiries, and report such families as neglect to avail themselves of the advantages thus offered. By command of Brig. Gen. H. W. Wessels, Andrew Stewart, Assistant Adjutant General. Plymouth, N. C., April 14th, 1864. And this is the order for running the churches: Notice. Until further orders church call will be sounded at the Provost Guard on Sundays, at fifteen minutes before 11 A.M., and at 2. 15 P. M. the call to be repeated promptly by the drums of the several regiments and detachments. The annoyance caused by entering and leaving the churches during the performance of Divine service, and by the practice of spitting on the floor is excessive, and it is hoped that these evils will be corrected without the necessity of individual reproof. By order of Brig. Gen. H. W. Wessels, D. F. Beegle, Lieut, A. D. C. &amp; A. A. A. G. Plymouth, N. C, April 11th, 1864. 1864-04-28_article_3 1864-04-28 article List of members of the next House of Representatives. List of members of the next House of Representatives. Below we give a correct list of the members of the next Confederate House of Representatives, prepared and courteously furnished us by Mr. De Louis Dalton, Asst-Clerk of the House. Those members whose names are marked thus [!] were members of the last Congress: Alabama. – 1st District, Thomas J. Foster;‖ 2, Wm R. Smith;‖ 3d, Williamson R. W. Cobb; 4th, Marcus H. Cruikshank; 5th, Francis S. Lyon;‖ 6th, Wm. P. Chilton; 7th, David Clopton;‖ 8th, James L. Pugh;‖ 9th, J. S. Dickinson. Arkansas. – 1st District, Felix I. Batson;‖ 2d, Rufus K. Garland; 3d, Augustus H. Garland;‖ 4th, Thomas B. Hanly.‖ Florida. – 1st District, St. George Rogers; 2d, Robert B. Hilton.‖ Georgia. – 1st District, Julian Hurtridge;‖ 2d, Wm. E. Smith; 3d, Mark H. Blandford; 4th, Clifford Anderson; 5th, J. T. Shewmake; 6th; J. H. Echots; 7th, James M. Smith; 8th, George N. Lester; 9th, H. P. Bell, 10th, Warren Aiken. Kentucky. – 1st District, Willis B. Machen;‖ 2d, George W. Triplett; 3d, Henry E. Read;‖ 4th Geo. W. Ewing;‖ 5th, James S. Chrisman;‖ 6th, Theodore. Le Burnett;‖ 7th, H. W. Bruce;‖ 8th, Humphrey Marshall; 9th, Ely M. Bruce; 10th, James W. Moore;‖ 11th Benjamin F. Bradly; 12th, John M. Eliott.‖ Louisiana. – 1st District, Charles J. Villere;‖ 2d, Chas. M. Conrad;‖ 3d, Duncan F. Kenner;‖ 4th, Lucins J. Dupre;‖ 5th, B. L. Hodge; 6th, John Perkins, Mississippi. – 1st District, John A. Orr; 2d, Wm. D. Holder,‖ 3d, Israel Welsh,‖ 4th, Henry C. Chambers;‖ 5th, Otho R. Singleton;‖ 6th, Ethelbert Barksdale;‖ 7th, J. T. Lampkin. Missouri. – To be elected May 2, 1864.) North Carolina. – 1st District, Wm. N. H. Smith;‖ 2d Robt. R. Bridgers;‖ 3d, J. T. Leach; 4th, Thos C. Faller; 5th, Josiah Turner; 6th, John A. Gilmer; 7th, James M. Leach; 8th, J. G. Ramsey; 9th, B. S. Gaithers, 10th, Geo. W. Logan. South Carolina. – 1st District, James M. Witherspoon; 2d, Wm. Porcher Miles;‖ 3d, Lucius M. Ayer;‖ 4th, Wm. D. Simpson;‖ 5th, James Farrar;‖ 6th, Wm. W. Boyee. Tennessee. – 1st District, Joseph B. Heiskell;‖ 2d, Wm. G. Swan‖; 3d, A. S. Colyer; 4th, John P. Murray; 5th, Henry S. Foote‖; 6th, E. A. Keeble; 7th, James McCollum; 8th, Thomas Menees;‖ 9th, John D. C. Atkins‖; 10th, John V. Wright‖; 11th, Daniel M. Currien‖; deceased.) Texas. – 1st District, John A. Wilcox‖, deceased;) 2d, C. C. Herbert‖; 3d, A.M. Branch; 4th, F. B. Sexton‖; 5th, A. R. Bayler; 6th, S. H. Morgan. Virginia. – 1st district, Robert L. Montague; 2d, R. H. Whitfield; 3d, Wms C. Wickham; 4th, T. S. Gholson; 5th, Thomas S. Bocock;‖ 6th, John Goode, Jr;‖ 7th, Wm. C. Rives; 8th; Daniel C. DeJarnett;‖ 9th, David Runsten;‖ 10th, F. W. M. Holliday; 11th, John R. Baldwin;‖ 12th, Waller R. Staples;‖ 13th, Fayette McMullen;‖ 4th, Samuel. A. Miller;‖ 15th, Robert Johnson;‖ 16th, Charles W. Russell.‖ 1864-04-28_article_4 1864-04-28 article NO HEADER Provisions at the South. – The Mobile Advertize, in an article on the prices and the currency, gives some facts that contrast strangely with the Richmond market. It says: Three weeks ago we paid $22. 50 per hundred for folder. Yesterday we bought the same article at $10. A month since bacon was selling at $6. The present price is $3. 50 to $4. 00. We paid a month ago for corn meal $10; yesterday we bought the best article for $5. 00. Even flour has tumbled down $75 a barrel, although it is still unreasonably dear. We notice a decided improvement in the vegetable and meat markets. Customers do not throng to the market as they used to do. – We saw meat in the market yesterday morning that has been there since Saturday – a certain sign of two important facts; that money is scarce, more prized, and not so readily parted with as formerly, and that the supply of meat is not short of the demand. The fishermen even are becoming more moderate. We have never seen the market so abundantly supplied as since the 1st of April. We boarded a large oyster boat at the wharf a few days ago. It had the remains of its cargo on board – 10, 000 oysters. We asked the price – $3 a basket. How many in a basket? Over 200. That is $1. 50 a hundred. The oyster-shops ask $3. 00 a hundred. The oyster boat had been two days at the wharf, and the skipper said he had been trying all day to sell out, and was debating between throwing them away or giving them to the Free Market. We advised the latter course by all means. Certainly there are signs of lower prices. And they appear thus early in the operation of the late laws of Congress. We did not anticipate any marked effect of this legislation until the fives were ruled out like the larger notes and the new tax law had gone into effect. If such things be in the green, what may we not hope for in the dry? – Money is going to be scarce, and prices are, in our opinion, sure to fall. The effect of the tax upon the planter has been marked in one respect. He is sending produce to market which he never thought it worth his while to take the trouble of before. Fodder, shucks, bacon, corn, peas, &amp; c., are coming in more freely than at any time during the war, and the prices of all these articles are coming down. Starvation is an obsolete idea. – There has always been enough in the South for man and beast. It only needed a golden key to unlock the treasure. Funding and taxation have supplied it. 1864-04-28_article_5 1864-04-28 article NO HEADER Impressive ceremony at Dalton. – A most interesting and impressive ceremony took place at the Episcopal Church in Dalton, Ga., on Wednesday last, in the confirmation, by Bishop Elliott, of four Generals of the Army of Tennessee, viz: Lieut. Gen. Hardee, and Gens. Govan, Shupe, and Strahl. The last named General was first received into the church by baptism, and then the rite of confirmation was administered to him with the others. The Atlanta Confederacy, referring to the impressive ceremony, says: An immense concourse, civil and military, witnessed the imposing spectacle, including the Commander in-Chief and nearly every general officer in the army. The Rev. Dr. Quintard and others of the Episcopal clergy officiated. The ceremony of \" the laying on of hands\" was performed by the eminent Bishop of this Dioceses the Rt. Rev. Bishop Stephen D. Elliott. 1864-04-28_article_6 1864-04-28 article Later from the North. A sure way to avoid service. – The Winston N. C.) Sentinel states that Alexander Ridings, of Forsyth county, committed suicide last week by hanging himself, to keep from going to the army. Later from the North. Let’s remove low freq items: d1864_lowFreq &lt;- d1864 %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort=TRUE) summary(d1864_lowFreq) ## word n ## Length:58774 Min. : 1.00 ## Class :character 1st Qu.: 1.00 ## Mode :character Median : 2.00 ## Mean : 61.63 ## 3rd Qu.: 8.00 ## Max. :278040.00 lowFreq &lt;- d1864_lowFreq %&gt;% filter(n &lt;= 1) summary(lowFreq) ## word n ## Length:28107 Min. :1 ## Class :character 1st Qu.:1 ## Mode :character Median :1 ## Mean :1 ## 3rd Qu.:1 ## Max. :1 Most of these low-frequency items are typos: knitr::kable(head(lowFreq, 15), table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) word n 0001850holstein482 1 00024 1 0003750 1 0004unshelled 1 0005 1 000aliens 1 000anne 1 000another 1 000april 1 000august 1 000banks’s 1 000butchers5500blacksmiths2200wheelrights2200teamsters505 1 000butler’s 1 000california50 1 000connecticut40 1 We can anti-join our corpus with lowFreq, which will remove them: d1864_clean &lt;- d1864 %&gt;% filter(type != &quot;ad_blank&quot;) d1864_clean &lt;- d1864_clean %&gt;% unnest_tokens(word, text) %&gt;% anti_join(lowFreq, by=&quot;word&quot;) %&gt;% group_by(id) %&gt;% count(word, sort=TRUE) # unfiltered: 2,815,144 # filtered (&gt;3): 2,749,078 Additionally, we need to remove stop words, but first we need to identify them. d1864_clean_FL &lt;- d1864_clean %&gt;% group_by(word) %&gt;% summarize(freq=sum(n)) %&gt;% arrange(desc(freq)) knitr::kable(head(d1864_clean_FL, 15), table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) word freq the 278040 of 163446 and 114985 to 98596 in 68070 a 67022 that 35265 for 32885 on 31679 was 30730 is 29191 at 27723 be 27673 by 26285 from 24300 To make things faster, you can remove top 50, 100, 150, 200 most frequent words, but this is a rather brutal way. Ideally, you want to curate your own stop word list that will be tuned to your texts. Below, I have taken top 500 words and manually removed everything that was meaningful (or, better, what I considered meaningful). Additionally, there are also NLP procedures that are designed to lemmatize words (i.e., reduce all words to their dictionary forms) and also do part-of-speech tagging, which allows to remove words categorically (for example, keeping only nouns, adjectives and verbs). word &lt;- c(&quot;the&quot;, &quot;of&quot;, &quot;and&quot;, &quot;to&quot;, &quot;in&quot;, &quot;a&quot;, &quot;that&quot;, &quot;for&quot;, &quot;on&quot;, &quot;was&quot;, &quot;is&quot;, &quot;at&quot;, &quot;be&quot;, &quot;by&quot;, &quot;from&quot;, &quot;his&quot;, &quot;he&quot;, &quot;it&quot;, &quot;with&quot;, &quot;as&quot;, &quot;this&quot;, &quot;will&quot;, &quot;which&quot;, &quot;have&quot;, &quot;or&quot;, &quot;are&quot;, &quot;they&quot;, &quot;their&quot;, &quot;not&quot;, &quot;were&quot;, &quot;been&quot;, &quot;has&quot;, &quot;our&quot;, &quot;we&quot;, &quot;all&quot;, &quot;but&quot;, &quot;one&quot;, &quot;had&quot;, &quot;who&quot;, &quot;an&quot;, &quot;no&quot;, &quot;i&quot;, &quot;them&quot;, &quot;about&quot;, &quot;him&quot;, &quot;two&quot;, &quot;upon&quot;, &quot;may&quot;, &quot;there&quot;, &quot;any&quot;, &quot;some&quot;, &quot;so&quot;, &quot;men&quot;, &quot;when&quot;, &quot;if&quot;, &quot;day&quot;, &quot;her&quot;, &quot;under&quot;, &quot;would&quot;, &quot;c&quot;, &quot;such&quot;, &quot;made&quot;, &quot;up&quot;, &quot;last&quot;, &quot;j&quot;, &quot;time&quot;, &quot;years&quot;, &quot;other&quot;, &quot;into&quot;, &quot;said&quot;, &quot;new&quot;, &quot;very&quot;, &quot;five&quot;, &quot;after&quot;, &quot;out&quot;, &quot;these&quot;, &quot;shall&quot;, &quot;my&quot;, &quot;w&quot;, &quot;more&quot;, &quot;its&quot;, &quot;now&quot;, &quot;before&quot;, &quot;three&quot;, &quot;m&quot;, &quot;than&quot;, &quot;h&quot;, &quot;o&#39;clock&quot;, &quot;old&quot;, &quot;being&quot;, &quot;left&quot;, &quot;can&quot;, &quot;s&quot;, &quot;man&quot;, &quot;only&quot;, &quot;same&quot;, &quot;act&quot;, &quot;first&quot;, &quot;between&quot;, &quot;above&quot;, &quot;she&quot;, &quot;you&quot;, &quot;place&quot;, &quot;following&quot;, &quot;do&quot;, &quot;per&quot;, &quot;every&quot;, &quot;most&quot;, &quot;near&quot;, &quot;us&quot;, &quot;good&quot;, &quot;should&quot;, &quot;having&quot;, &quot;great&quot;, &quot;also&quot;, &quot;over&quot;, &quot;r&quot;, &quot;could&quot;, &quot;twenty&quot;, &quot;people&quot;, &quot;those&quot;, &quot;e&quot;, &quot;without&quot;, &quot;four&quot;, &quot;received&quot;, &quot;p&quot;, &quot;then&quot;, &quot;what&quot;, &quot;well&quot;, &quot;where&quot;, &quot;must&quot;, &quot;says&quot;, &quot;g&quot;, &quot;large&quot;, &quot;against&quot;, &quot;back&quot;, &quot;000&quot;, &quot;through&quot;, &quot;b&quot;, &quot;off&quot;, &quot;few&quot;, &quot;me&quot;, &quot;sent&quot;, &quot;while&quot;, &quot;make&quot;, &quot;number&quot;, &quot;many&quot;, &quot;much&quot;, &quot;give&quot;, &quot;1&quot;, &quot;six&quot;, &quot;down&quot;, &quot;several&quot;, &quot;high&quot;, &quot;since&quot;, &quot;little&quot;, &quot;during&quot;, &quot;away&quot;, &quot;until&quot;, &quot;each&quot;, &quot;5&quot;, &quot;year&quot;, &quot;present&quot;, &quot;own&quot;, &quot;t&quot;, &quot;here&quot;, &quot;d&quot;, &quot;found&quot;, &quot;reported&quot;, &quot;2&quot;, &quot;right&quot;, &quot;given&quot;, &quot;age&quot;, &quot;your&quot;, &quot;way&quot;, &quot;side&quot;, &quot;did&quot;, &quot;part&quot;, &quot;long&quot;, &quot;next&quot;, &quot;fifty&quot;, &quot;another&quot;, &quot;1st&quot;, &quot;whole&quot;, &quot;10&quot;, &quot;still&quot;, &quot;among&quot;, &quot;3&quot;, &quot;within&quot;, &quot;get&quot;, &quot;named&quot;, &quot;f&quot;, &quot;l&quot;, &quot;himself&quot;, &quot;ten&quot;, &quot;both&quot;, &quot;nothing&quot;, &quot;again&quot;, &quot;n&quot;, &quot;thirty&quot;, &quot;eight&quot;, &quot;took&quot;, &quot;never&quot;, &quot;came&quot;, &quot;called&quot;, &quot;small&quot;, &quot;passed&quot;, &quot;just&quot;, &quot;brought&quot;, &quot;4&quot;, &quot;further&quot;, &quot;yet&quot;, &quot;half&quot;, &quot;far&quot;, &quot;held&quot;, &quot;soon&quot;, &quot;main&quot;, &quot;8&quot;, &quot;second&quot;, &quot;however&quot;, &quot;say&quot;, &quot;heavy&quot;, &quot;thus&quot;, &quot;hereby&quot;, &quot;even&quot;, &quot;ran&quot;, &quot;come&quot;, &quot;whom&quot;, &quot;like&quot;, &quot;cannot&quot;, &quot;head&quot;, &quot;ever&quot;, &quot;themselves&quot;, &quot;put&quot;, &quot;12&quot;, &quot;cause&quot;, &quot;known&quot;, &quot;7&quot;, &quot;go&quot;, &quot;6&quot;, &quot;once&quot;, &quot;therefore&quot;, &quot;thursday&quot;, &quot;full&quot;, &quot;apply&quot;, &quot;see&quot;, &quot;though&quot;, &quot;seven&quot;, &quot;tuesday&quot;, &quot;11&quot;, &quot;done&quot;, &quot;whose&quot;, &quot;let&quot;, &quot;how&quot;, &quot;making&quot;, &quot;immediately&quot;, &quot;forty&quot;, &quot;early&quot;, &quot;wednesday&quot;, &quot;either&quot;, &quot;too&quot;, &quot;amount&quot;, &quot;fact&quot;, &quot;heard&quot;, &quot;receive&quot;, &quot;short&quot;, &quot;less&quot;, &quot;100&quot;, &quot;know&quot;, &quot;might&quot;, &quot;except&quot;, &quot;supposed&quot;, &quot;others&quot;, &quot;doubt&quot;, &quot;set&quot;, &quot;works&quot;) sWordsDF &lt;- data.frame(word) d1864_clean_minusSW &lt;- d1864_clean %&gt;% anti_join(sWordsDF, by=&quot;word&quot;) dim(d1864_clean) ## [1] 1765298 3 # 1,759,828 dim(d1864_clean_minusSW) ## [1] 1162644 3 # 1,159,214 9.2.2 TF-IDF From Wikipedia: In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf. Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document’s relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification. One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. df_TF_IDF &lt;- d1864_clean_minusSW %&gt;% # d1864_clean, d1864_clean_minusSW bind_tf_idf(word, id, n) %&gt;% arrange(desc(tf_idf)) %&gt;% ungroup knitr::kable(head(df_TF_IDF, 15), table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) id word n tf idf tf_idf 1864-07-25_article_6 suicides 2 0.2500000 9.589941 2.397485 1864-09-12_advert_131 leonard’s 3 0.2307692 8.896793 2.053106 1864-02-05_article_19 sissy 3 0.2000000 9.589941 1.917988 1864-01-28_article_49 generality 1 0.2500000 7.644030 1.911008 1864-01-11_advert_105 blacking 6 0.2307692 8.203646 1.893149 1864-09-12_advert_131 pills 3 0.2307692 8.203646 1.893149 1864-12-08_article_70 council 2 0.4000000 4.508536 1.803414 1864-12-12_advert_143 alf 2 0.1818182 9.589941 1.743626 1864-09-27_article_95 calendar 2 0.2857143 6.006422 1.716120 1864-03-22_article_71 apartments 2 0.2222222 7.644030 1.698673 1864-12-29_article_42 dishonest 1 0.2500000 6.756727 1.689182 1864-09-10_article_61 mobile 2 0.5000000 3.373334 1.686667 1864-05-05_article_17 ling 1 0.2000000 8.203646 1.640729 1864-09-26_article_63 hoy 1 0.2000000 8.203646 1.640729 1864-03-30_article_50 telegrams 2 0.3333333 4.810817 1.603606 articleID = &quot;1864-07-25_article_6&quot; filter(df_TF_IDF, id==articleID) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 6 × 6 ## id word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1864-07-25_article_6 suicides 2 0.25 9.59 2.40 ## 2 1864-07-25_article_6 france 2 0.25 4.35 1.09 ## 3 1864-07-25_article_6 suicide 1 0.125 6.50 0.812 ## 4 1864-07-25_article_6 committed 1 0.125 3.09 0.387 ## 5 1864-07-25_article_6 persons 1 0.125 2.61 0.327 ## 6 1864-07-25_article_6 take 1 0.125 2.26 0.283 d1864$text[d1864$id==articleID] ## [1] &quot;Suicides in France -- More than ten suicides take place every day in France; last year 4, 000 persons committed suicide.&quot; 9.3 Clustering 9.3.1 Hierarchical clustering Clustering is a method to break items into closely related groups—clusters. The code below show how our data can be broken into clusters with hierarchical clustering, using distance matrices. Hierarchical clustering has rather high precision, but sensitive to outliers and computationally expensive, which makes it nearly unusable with large datasets. K-means clustering is more suitable for large datasets, but struggles with uniform data (for example). In both cases you have to define the number of clusters. In what follows, we take our TF-IDF data, sample it, and run cluster analysis on a small sample. The chunk below simply prepares our data for analysis: # RANDOMLY SELECT N ITEMS set.seed(48965) N = 100 sampledIDs &lt;- sample(unique(df_TF_IDF$id), N) sample_d1864_tfidf &lt;- df_TF_IDF %&gt;% filter(id %in% sampledIDs) %&gt;% select(id, word, tf_idf) # CONVERT INTO DTM MATRIX colnames(sample_d1864_tfidf) &lt;- c(&quot;document&quot;, &quot;term&quot;, &quot;count&quot;) sample &lt;- tibble(sample_d1864_tfidf) %&gt;% cast_dtm(document, term, count) sample_matrix &lt;- as.matrix(sample) # CONVERT INTO REGULAR MATRIC AND CALCULATE DISTANCES distanceMatrix &lt;- dist.cosine(sample_matrix) # from library(stylo) distanceMatrixHC &lt;- as.dist(distanceMatrix) Now we can do the actual clustering. There are several clustering/linkage methods that can be used for clustering, and it usually depends on your goals. From ?hclust: “A number of different clustering methods are provided. Ward’s minimum variance method aims at finding compact, spherical clusters. The complete linkage method finds similar clusters. The single linkage method (which is closely related to the minimal spanning tree) adopts a ‘friends of friends’ clustering strategy. The other methods can be regarded as aiming for clusters with characteristics somewhere between the single and complete link methods. Note however, that methods”median” and “centroid” are not leading to a monotone distance measure, or equivalently the resulting dendrograms can have so called inversions or reversals which are hard to interpret, but note the trichotomies in Legendre and Legendre (2012).” As a rule of thumb: you want balanced trees when you want an even number of items assigned to each cluster. Unbalanced trees are useful for finding outliers — with this method you can find which items you might want to remove in order to achieve better clustering. You can find additional explanations here: https://towardsdatascience.com/https-towardsdatascience-com-hierarchical-clustering-6f3c98c9d0ca. # THE FOLLOWING IS THE ACTUAL CLUSTERING clustered.data.ward &lt;- hclust(distanceMatrixHC, method = &quot;ward.D&quot;) clustered.data.complete &lt;- hclust(distanceMatrixHC, method = &quot;complete&quot;) clustered.data.average &lt;- hclust(distanceMatrixHC, method = &quot;average&quot;) clustered.data.single &lt;- hclust(distanceMatrixHC, method = &quot;single&quot;) str(clustered.data.ward) ## List of 7 ## $ merge : int [1:99, 1:2] -23 -2 -58 -99 -52 -3 -51 -60 -86 -17 ... ## $ height : num [1:99] 0.699 0.714 0.732 0.733 0.758 ... ## $ order : int [1:100] 51 23 72 86 52 58 89 32 60 61 ... ## $ labels : chr [1:100] &quot;1864-10-10_printrun_6&quot; &quot;1864-04-16_article_26&quot; &quot;1864-10-01_ad-blank_69&quot; &quot;1864-03-23_article_58&quot; ... ## $ method : chr &quot;ward.D&quot; ## $ call : language hclust(d = distanceMatrixHC, method = &quot;ward.D&quot;) ## $ dist.method: NULL ## - attr(*, &quot;class&quot;)= chr &quot;hclust&quot; plot(clustered.data.ward, labels=FALSE, main=&quot;Ward&quot;) abline(h=1.25, col=&quot;blue&quot;, lty=3) rect.hclust(clustered.data.ward, k=10, border=&quot;red&quot;) You can use rect.hclust() either with h= — which will cut tree at a given height, thus determining the number of clusters; or, with k= — which will cut tree into a given number of clusters. 9.4 PCA viz for HCLUST Note that PCA (principal component analysis) and clustering are not the same, but PCA can be also used to visualize hierarchical clustering: library(factoextra) set.seed(1) distanceMatrixHC.scaled &lt;- scale(distanceMatrixHC) hc.cut &lt;- hcut(distanceMatrixHC.scaled, k = 3, hc_method = &quot;ward.D&quot;) fviz_cluster(hc.cut, labelsize=0, ellipse.type = &quot;convex&quot;) plot(clustered.data.complete, labels=FALSE, main=&quot;Complete&quot;) plot(clustered.data.average, labels=FALSE, main=&quot;Average&quot;) plot(clustered.data.single, labels=FALSE, main=&quot;Single&quot;) hclust() creates an object from which you can extract further information. For example, we can use cutree() function to extract clustering information. You can use cutree() either with h= — which will cut tree at a given height, thus determining the number of clusters; or, with k= — which will cut tree into a given number of clusters. clusters_named_vector &lt;- cutree(clustered.data.ward, k=10) clusters_df &lt;- stack(clusters_named_vector) colnames(clusters_df) &lt;- c(&quot;cluster&quot;, &quot;id&quot;) clusters_df &lt;- clusters_df %&gt;% select(id, cluster) knitr::kable(head(clusters_df, 15), table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) id cluster 1864-10-10_printrun_6 1 1864-04-16_article_26 2 1864-10-01_ad-blank_69 3 1864-03-23_article_58 4 1864-08-23_article_36 4 1864-05-09_advert_47 1 1864-01-06_article_114 4 1864-04-14_article_39 2 1864-12-12_advert_116 4 1864-10-12_article_14 4 1864-10-27_advert_59 4 1864-07-11_advert_55 5 1864-03-09_article_53 6 1864-09-12_advert_4 7 1864-01-07_article_68 4 We can then left_join these clustering results with the original table and manually check if our clustering makes sense. d1864_clustering &lt;- d1864 %&gt;% left_join(clusters_df, by = &quot;id&quot;) Let’s print out a few clusters: cluster &lt;- d1864_clustering %&gt;% filter(cluster == 9) %&gt;% select(text) knitr::kable(cluster, table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) text Twelve Hundred and fifty Dollars reward. – A reward of two Hundred and fifty Dollars will be paid for the delivery to me, in this city, of each one of the following Slaves, who have absconded from the Carbon Hill Mines, in this county, during the past six months: Charles, a mulatto, about five feet nine inches in height, thirty-two years old, not stout, with very noticeable grey eyes; from Orange county. Daniel, a dark brown negro, about five feet eight inches in height, nineteen or twenty years old, very sprightly countenance, and very talkative. William, dark brown or black, five feet eight or nine inches in height, very slightly made, thirty-two or thirty-four years old, very quiet in his demeanor; from Gloucester county Virginia. Festus, dark brown, five feet seven or eight inches in height, well made, pleasant countenance and good address, twenty-two to twenty-four years of age. Lewis, black, five feet seven or eight inches in height, square built, but not very stout; twenty-eight or thirty years of age; limps slightly. The two latter have relations living in Richmond, and have been employed for some years in Manchester. John J. Werth, Agent. de 2 – 2aw4w 50 dollars reward. – Ran away from my farm, near White Oak Swamp bridge, my negro boy Mitchell. Said negro is about 18 years old, black, five feet high, has a scar over the left temple; had on when last seen a slouched hat and black woolen pants; supposed to be lurking about Richmond. The above reward will be given if delivered at my farm, or Mr. Jack Fisher’s. Loftin D. Allen. Henrico county. je 10 – cod3t One hundred and fifty dollars reward. – Ran away, on Monday, 17th instant, a Negro Boy, sixteen years old; about five feet eight or nine inches high, very nearsighted, and black, with a scar on the left cheek, under the eye. He has been seen in the city every day. The above reward will be paid for his delivery at the New Richmond Theatre, corner Seventh and Broad streets. oc 21 – ts 50 dollars reward. – Ran away on Monday, the 18th inst, from Mrs Lucy C Binford, , a negro girl about eighteen years old, well grown, dark brown, slender made — She was at the head of Mechanicsville Turnpike, Hanover county, and she is probably lurking about that neighborhood. She has a husband living with Dr John G Lempkis. I will pay the above reward if delivered to me, or secured in jail so that I can get her. J. B. Keesee, Adm’r of W. M. A. Binford, dec’d. Henrico co, July 25, 1864 jy 26 – 2t 500 dollars reward. – I will give the above reward for the apprehension of, and delivery to me; at my office, or his lodgement in jail so that I can get him) of my servant boy George Henry Ray. Said boy is a bright mulatto, about five feet high, wooly hair, has a swaggering walk, speaks very quick, and has a very sullen look. He may possibly be lurking about the city, as he has acquaintances in every part of it, but my impression is that he is endeavoring to get to Fredericksburg, from thence to Stafford, to the farm of the late Major John Seddon where he has a father.) in order to pass the lines. Soldiers in the camps are cautioned against employing said boy. M A Blackman, Surgeon and Dentist, No. 83 Main street, Richmond, Va. As he has been accompanying the to the hospitals, he has a pass from Capt Coke, requesting the guard to pass him unmolested. M A B. jy 22 – ts Ran away – From the gravel train on Sunday, 28th ult, four negroes, named Ned, Frederick, Efford, and Albert, hired of Mrs. A C Isbell, of Cumberland county. Efford and Albert are of a bright gingerbread color, 5 feet 10 inches high; Ned and Frederick are of dark complexion, stout, 5 feet 7 inches high. The usual reward will be paid for their apprehension. C G Talbott. Supt Richmond and Denville R R. mh 6 – ts 200 dollars reward. – Ranaway from the subscriber about the 17th of January last, two slaves, named Doctor and Raleigh commonly called Flem. Both are black, quick, active man. – Doctor is about 19 years old, had his left hand hurt in an apple mill and scarred on the outside, and has lost one or more of his nails from that hand. He had on a red flannel shirt when he left. Flem is left handed, and is about 17 years of age. I will give $100 a piece for the delivery to me, or to some jail from which I can get them, of the said slaves, if caught out of the county of Charlotte, or $50 a piece if apprehended in the county. I believe they are passing as free men, and are trying to get employment on the Richmond and Danville, or Southside Railroad. Address, Henry E. Edmunds, Moesingford P. O., Charlotte county. Va. fe 16 – 1m 200 dollars reward. – Ran off on 19th March a negro woman named Creasy, a very small black woman, thick lips, and large nose, and very short spoken. Carried off with her two new striped homespun yarn dresses, one brown and the other black. The above reward will be given for her apprehension if taken out of the county, at one hundred if taken in the county, in either case to be delivered to me or secured in jail so I get her again. William Priddy. Negro Foot P. O., Hanover co, Va. ap 2 – 6t 300 dollars reward – will be paid for the delivery of my servant girl Carellec, who left on Tuesday morning, the 1st instant. She is black, about 18 years of age, well grown, rather stout, has round face and thick lips. A Rodeker, Druggist, No. 19 Main st. au 6 – Five hundred dollars reward. – Ran away from the Richmond Arsenal, where he was hired, about the 20th of July, my negro man, Peter miles, sometimes called Peter Redd. He is twenty-two or twenty-three years of age, five feet eight or nine inches high, gingerbread color, rather a long face, nose rather long and flat, and carries himself very erect. When he left the arsenal be wore a grey jacket and cap. He is no doubt making an effort to pass through our lines. I will give the above reward if caught beyond the limits of the city, or two hundred and fifty dollars if caught within the corporate limits and secured in any jail so that I get him. Neal McCURDY. au 10 – 6t* Ran away from my Farm, at the Half-way House, on the Richmond and Petersburg railroad, Chesterfield country, my man Richard. He left my farm last Tuesday morning, the 9th instant, and had on when he left a pair of dark pants, white cotton shirt, and had on a pair of shoes, no coat nor hat. He is about twenty or twenty-one years old, five feet six or seven inches high, black, has a small moustache, and speaks slow. I bought him last April, of Lee &amp; Bowman, in Richmond. He formerly belonged to Miss Margaret Bottom, of Amelia Courthouse. He has a wife at or near Amelia Courthouse, and may be trying to go there. He was last seen near the Half-Way Station. I will pay a liberal reward if caught and put in jail, or delivered to me. Address J. M. Wolff, 64 Main street, Richmond, Va., or Proctor’s Creek, Chesterfield county. au 17 – 6t* Ranaway. – $100 reward. – From the subscriber, in 10th March, 1864, my men Washington, again about 26 years, about complexion, large mouth will be given if taken out of the State, and if taken in it, and secured so I get him. My address in Fork Union, Fluvanna co, Va., Samuel R. Pellet. ap 26 – 1aw4t 100 Dollars reward. – Ranaway from the Midlothian Coal Mines, a negro man named Joe, or Joe Hampton. He is about 25 years old, of dark brown color, spare made, about 5 feet 10 inches high, with rather large eyes, and somewhat wild expression of countenance, though generally smiling when spoken to. He was bought in January last of Mr C C Burton, near Petersburg, where his friends and connexions are, and he is probably in the neighborhood of that place. The above reward will be paid for his apprehension and delivery in any jail, or to the agent of the Company, at their mines, or in Richmond. my 12 – ts Ruanaway. – $100 reward will be paid for the delivery to S N Davis &amp; Co. of a negro boy named John. He is about 18 years old, gingerbread color; he had on a black felt hat, boots tipped on the toes, and gray pantaloons, when he left Friday. He was raised in Albemarle, by Dr. T J Cook Geo. Turner mh 14 – 6t cluster &lt;- d1864_clustering %&gt;% filter(cluster == 7) %&gt;% select(text) knitr::kable(cluster, table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) text For rent – A store neatly fitted up, and the best stand in the town. Apply to John Perry, Ashland Hotel. fe 6 – 2t* For Rent – Two nicely furnishes rooms, suitable for ladies and gentlemen, or gentlemen, with use of parlor and kitchen. Apply at the corner house on the north corner of Marshall and Adams streets. mh 7 – 1t* Shockoe Mill, Seventeenth street, across the Dock. Wheat and Corn ground on tell. For sale – Nice New Flour at $1. 25 per pound. Corn Meal at $55 per bushel. au 4 – 10t* For Rent and Sale. For Rent, a furnished House in a pleasant part of the city. For terms, address \" E. S.,\" with references, Dispatch office. se 12 – 3t* For Rent, a very desirable Dwelling-House, with eight rooms, a spacious yard, out-houses, &amp; c., on Nineteenth street, one door from Grace. Possession given on 1st October. Apply to Mrs. Branch, Franklin street, between Eighteenth and Nineteenth. se 12 – eod3t* cluster &lt;- d1864_clustering %&gt;% filter(cluster == 5) %&gt;% select(text) knitr::kable(cluster, table.attr = &quot;class=\\&quot;striped\\&quot;&quot;, format=&quot;html&quot;) text Silk culture. Some of our citizens are disposed to turn their attention to the culture of silk. It would afford a new and most profitable employment to women and children. Many years ago there was a great deal of silk produced in this State. The morus multicaulis was extensively cultivated, and almost every house had its room devoted to silk worms, and their feeding and spinning. The business was abandoned, however, and the multicaulis trees were taken up by the roots, and cast out upon the highways to die. Yet there are no doubt some scattering bushes which escaped the general massacre which occurred when it was concluded that the enterprise was unprofitable. Moreover, it may be that some person having faith in the old adage that everything — the most useless, even — comes in use once in seven years, may have saved and kept on hand a small stock of the silk worms. There is now an enquiry for them, and if anybody has ever so small a family, their eggs will sell almost as high as hen’s eggs. The have but to announce that they have the genuine silk worm, to invite orders from every direction. 50 Dollars reward. – Stolen from the residence of James Sinton, Jr, on Franklin st, on Sunday, the 8th last, one new large black Silk Circular, trimmed with white buttons. Also, one Stella Shawl. The above reward will be given for the return of the articles to the \" Illustrated News Office,\" and no questions asked. my 9 – 3t Blockade Goods. – Mantle Depot, 60 Main, between 14th and 15th streets. – Ladies’silk parasols, large sizes; white silk illusion; superior quality bonnet ribbons; blond laces; blk silk lacer; blk English crapes; blk English crapes; blk dress silk; bonnet frames; checked muslins, jaconets; cambrics; bleached cotton; extra heavy English cotton hose; lists hose; crape collars; palm leaf fans; black alpacas; castile soap; Ezekiel’s hair tonic; tooth brushes; pomades; collogue, &amp; c, &amp; c. Mantle Depot, 69 Main, bet 14th and 15th sts jy 11 – 3t* 9.4.1 Determining the optimal number of clusters: “Elbow Method” and “Average Silhouette Method” In a nutshell, we repeatedly run clustering, increasing the number of clusters by one, and calculate the total within-cluster sum of square (wss). We then plot the curve of wss and look for a point in the curve with the sharpest bend (hence the “elbow”), which is considered to be an indicator of the appropriate number of clusters. library(factoextra) can perform this with a single command: set.seed(786) fviz_nbclust(as.matrix(distanceMatrixHC), FUN = hcut, k.max=10, method = &quot;wss&quot;) Ideally, we sould have something like L or V. Here, perhaps, 3 is our optimal number. We can try another method — average silhouette method (which is also easily callable from library(factoextra)). Like with elbow method, we run clustering multiple times but here we measures the quality of a clustering, but determining how well each object lies within its cluster. A high average silhouette width indicates a good clustering. set.seed(786) fviz_nbclust(as.matrix(distanceMatrixHC), FUN = hcut, k.max = 10, method = &quot;silhouette&quot;) More on hierarchical clustering: “Hierarchical Cluster Analysis”, in in U of Cincinnati Business Analytics R Programming Course http://uc-r.github.io/hc_clustering. 9.5 K-means clustering Let’s get a different sample from our data. With k-means clustering we can run on more data: # RANDOMLY SELECT N ITEMS set.seed(48965) N = 1000 sampledIDs &lt;- sample(unique(df_TF_IDF$id), N) sample_d1864_tfidf &lt;- df_TF_IDF %&gt;% filter(id %in% sampledIDs) %&gt;% select(id, word, tf_idf) # CONVERT INTO DTM MATRIX colnames(sample_d1864_tfidf) &lt;- c(&quot;document&quot;, &quot;term&quot;, &quot;count&quot;) sample &lt;- tibble(sample_d1864_tfidf) %&gt;% cast_dtm(document, term, count) sample_matrix &lt;- as.matrix(sample) # CONVERT INTO REGULAR MATRIC AND CALCULATE DISTANCES distanceMatrix &lt;- dist.cosine(sample_matrix) # from library(stylo) distanceMatrixKM &lt;- as.dist(distanceMatrix) kmeansClusters &lt;- kmeans(distanceMatrixKM, centers=5, nstart=25) str(kmeansClusters) ## List of 9 ## $ cluster : Named int [1:1000] 2 2 2 2 2 2 5 2 2 2 ... ## ..- attr(*, &quot;names&quot;)= chr [1:1000] &quot;1864-01-28_article_49&quot; &quot;1864-10-10_printrun_6&quot; &quot;1864-10-10_advert_3&quot; &quot;1864-06-22_article_22&quot; ... ## $ centers : num [1:5, 1:1000] 1 0.997 1 1 1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:1000] &quot;1864-01-28_article_49&quot; &quot;1864-10-10_printrun_6&quot; &quot;1864-10-10_advert_3&quot; &quot;1864-06-22_article_22&quot; ... ## $ totss : num 2199 ## $ withinss : num [1:5] 9.03 1313.82 84.42 186.05 9.94 ## $ tot.withinss: num 1603 ## $ betweenss : num 595 ## $ size : int [1:5] 12 806 37 121 24 ## $ iter : int 3 ## $ ifault : int 0 ## - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; We can extract cluster information in the same manner as with hclust() object: kmeans_clusters_df &lt;- stack(kmeansClusters$cluster) colnames(kmeans_clusters_df) &lt;- c(&quot;cluster&quot;, &quot;id&quot;) kmeans_clusters_df &lt;- kmeans_clusters_df %&gt;% select(id, cluster) head(kmeans_clusters_df) ## id cluster ## 1 1864-01-28_article_49 2 ## 2 1864-10-10_printrun_6 2 ## 3 1864-10-10_advert_3 2 ## 4 1864-06-22_article_22 2 ## 5 1864-04-16_article_26 2 ## 6 1864-03-07_advert_112 2 We can visualize the results of our kmeans clustering using fviz_cluster() (from factoextra). Our data is multidimensional — each word in our matrix is a single dimension, so this function will perform principal component analysis (PCA) and plot data according to the first two principal components that explain majority of the variance in our dataset. set.seed(786) fviz_cluster(kmeansClusters, data = distanceMatrix, labelsize = 0) 9.6 Determining the optimal number of clusters: “Elbow Method” and “Average Silhouette Method” In a nutshell, we repeatedly run clustering, increasing the number of clusters by one, and calculate the total within-cluster sum of square (wss). We then plot the curve of wss and look for a point in the curve with the sharpest bend (hence the “elbow”), which is considered to be an indicator of the appropriate number of clusters. library(factoextra) can perform this with a single command: set.seed(786) fviz_nbclust(as.matrix(distanceMatrix), kmeans, k.max=20, method = &quot;wss&quot;) Ideally, we should have something like L or V. Here results do not seem to be very helpful. We can try another method — average silhouette method (which is also easily callable from library(factoextra)). Like with elbow method, we run clustering multiple times but here we measures the quality of a clustering, bu determining how well each object lies within its cluster. A high average silhouette width indicates a good clustering. set.seed(786) fviz_nbclust(as.matrix(distanceMatrix), kmeans, k.max = 20, method = &quot;silhouette&quot;) Let’s try to visualize our clusters again: set.seed(786) kmeansClusters &lt;- kmeans(distanceMatrixKM, centers=5, nstart=25) fviz_cluster(kmeansClusters, data = distanceMatrix, labelsize = 0) Here is, however, an example of where k-means clustering may/can fail: we gave a different starting point to the algorithm and set.seed(786) kmeansClusters &lt;- kmeans(distanceMatrixKM, centers=5, nstart=50) fviz_cluster(kmeansClusters, data = distanceMatrix, labelsize = 0) More information, see: “K-means Cluster Analysis”, in U of Cincinnati Business Analytics R Programming Course https://uc-r.github.io/kmeans_clustering. https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a 9.7 Other “clustering” methods Although not cluster analysis techniques strictly speaking, PCA (principal component analysis) and MDS (multi-dimensional scaling) are used in similar ways. We will touch upon these in the context of stylometry. 9.8 Topic Modeling 9.8.1 Topics? 9.8.1.1 Example 1 Thursday, March 27, 1862 LIGHT ARTILLERY —I am authorized by the Governor of Virginia to raise a Company of Light Artillery for the war. All those desirous of enlisting in this the most effective arm of the service, would do well to call at once at the office of Johnson &amp; Guigon, Whig Building. Uniforms and subsistence furnished. A. B. GUIGON. mh 25—6t 9.8.1.2 Example 2 Wednesday, August 17, 1864 Royal Marriages. —There is a story circulated in Germany, and some in Paris, that the match between the heir-apparent of the Imperial throne of Russia and the Princess Dagmar of Denmark having been definitively broken off, another is in the course of negotiation between His Imperial Highness and the Princess Helens of England. 9.8.1.3 Example 3 Monday, June 22, 1863 NEWS FROM EUROPE. The steamship Scotia arrived at New York on Thursday from Europe, with foreign news to the 7th inst. The news is not important. The Confederate steamer Lord Clyde was searched by order of the British Government, but nothing contraband being found on board her she was permitted to sail. The Russians have been defeated near Grochoury by the Polish insurgents. The three Powers have sent an earnest note to Russia, asking for a representative Government, a general amnesty, and an immediate cessation of hostilities in Poland. 9.8.2 Getting to code We can start with our preprocessed variable d1864_clean, which is essentially a cumulative frequency list for all articles. library(tm) library(topicmodels) library(tictoc) head(d1864_clean_minusSW) ## # A tibble: 6 × 3 ## # Groups: id [5] ## id word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1864-09-12_advert_30 mrs 307 ## 2 1864-09-12_advert_30 miss 251 ## 3 1864-04-18_article_10 mr 84 ## 4 1864-08-27_article_41 slave 79 ## 5 1864-11-24_article_46 mr 71 ## 6 1864-04-26_article_8 enemy 64 d1864_dm &lt;- d1864_clean_minusSW %&gt;% cast_dtm(id, word, n) d1864_dm ## &lt;&lt;DocumentTermMatrix (documents: 14617, terms: 30391)&gt;&gt; ## Non-/sparse entries: 1162644/443062603 ## Sparsity : 100% ## Maximal term length: 32 ## Weighting : term frequency (tf) Training a model. NB: eval=FALSE setting will prevent from running this chunk when you Knit the notebook; but you can still execute it within the notebook, when you run chunks individually tic() d1864_lda &lt;- LDA(d1864_dm, k = 4, control = list(seed = 1234)) d1864_lda toc() #A LDA_VEM topic model with 2 topics. #35.962 sec elapsed #A LDA_VEM topic model with 4 topics. #72.545 sec elapsed Do not run this! tic() kVal &lt;- 25 d1864_lda_better &lt;- LDA(d1864_dm, k=kVal, control=list(seed=1234)) toc() #A LDA_VEM topic model with 20 topics. #1261.087 sec elapsed (21 minutes) #A LDA_VEM topic model with 25 topics. #1112.262 sec elapsed (18 minutes) Save/load the model, so that there is no need to regenerate it every time. #d1864_lda_vem_25t_better &lt;- d1864_lda_better #save(d1864_lda_vem_25t_better, file=&quot;./files/data/d1864_lda_vem_25t_better.rda&quot;) #load(file=&quot;./data/d1864_lda_vem_25t_better.rda&quot;) load(file=&quot;./files/data/d1864_lda_vem_25t_better.rda&quot;) lda_model &lt;- d1864_lda_vem_25t_better corpus &lt;- d1864 From this point on, the code should simply run — if you rename your own model produced above to lda_model. 9.9 Per-topic-per-word probabilities (beta) lda_model_better_topic_term_prob &lt;- tidy(lda_model, matrix=&quot;beta&quot;) lda_model_better_topic_term_prob %&gt;% filter(term == &quot;bonds&quot;) %&gt;% arrange(desc(beta)) ## # A tibble: 25 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 8 bonds 3.53e- 2 ## 2 9 bonds 5.05e- 4 ## 3 24 bonds 3.92e- 4 ## 4 11 bonds 2.34e-13 ## 5 2 bonds 2.13e-13 ## 6 1 bonds 3.67e-16 ## 7 16 bonds 2.01e-20 ## 8 14 bonds 3.08e-21 ## 9 18 bonds 3.87e-22 ## 10 3 bonds 2.00e-22 ## # … with 15 more rows NB: This step may throw an error. The error seems a bit cryptic, but restarting R (without saving workspace) seems to help. (beta stands for term-per-topic probability). top_terms &lt;- lda_model_better_topic_term_prob %&gt;% group_by(topic) %&gt;% top_n(15, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) head(top_terms) ## # A tibble: 6 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 york 0.0113 ## 2 1 mr 0.0102 ## 3 1 lincoln 0.00915 ## 4 1 union 0.00827 ## 5 1 states 0.00809 ## 6 1 washington 0.00689 library(ggplot2) top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend=FALSE) + facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip() Here it is in a bit of a better quality: Topic-per-document probabilities: this object will tell us to which topics documents belong (and to what extent): (gamma stands for topic-per-document probability). lda_model_topic_doc_prob &lt;- tidy(lda_model, matrix=&quot;gamma&quot;) lda_model_topic_doc_prob ## # A tibble: 356,150 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1864-09-12_advert_30 1 0.0000115 ## 2 1864-04-18_article_10 1 0.709 ## 3 1864-08-27_article_41 1 0.0000581 ## 4 1864-11-24_article_46 1 0.0614 ## 5 1864-04-26_article_8 1 0.0000152 ## 6 1864-05-09_article_1 1 0.0000152 ## 7 1864-05-14_article_23 1 0.139 ## 8 1864-05-16_article_6 1 0.122 ## 9 1864-06-27_article_2 1 0.0000270 ## 10 1864-04-16_article_15 1 0.620 ## # … with 356,140 more rows Pick a document and print out topics it belongs to (from the most prominent to less prominent). (Hint: use the object we just created &gt; filter &gt; arrange). # your code here Top N documents per topic: this will create an object with top N documents per each topic. N = 10 top_docs &lt;- lda_model_topic_doc_prob %&gt;% group_by(topic) %&gt;% top_n(N, gamma) %&gt;% ungroup() %&gt;% arrange(topic, -gamma) top_docs ## # A tibble: 285 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1864-09-14_article_65 1 0.992 ## 2 1864-08-31_article_103 1 0.989 ## 3 1864-09-08_article_34 1 0.984 ## 4 1864-04-19_article_30 1 0.980 ## 5 1864-03-23_article_68 1 0.977 ## 6 1864-06-03_article_12 1 0.965 ## 7 1864-12-29_article_47 1 0.957 ## 8 1864-03-10_article_118 1 0.956 ## 9 1864-06-08_article_15 1 0.940 ## 10 1864-09-01_article_65 1 0.939 ## # … with 275 more rows Now that we have IDs of representative documents, we can check those documents one by one, but let’s do something else first: topic-title function—–it is not really necessary, but it will combine together topic number (from the model) and its top words, which can be used for graphs. topicVar &lt;- function(top, topTerms){ topicVar &lt;- topTerms %&gt;% filter(topic == top) %&gt;% arrange(-beta) vals = paste(topicVar$term, collapse=&quot;, &quot;) as.String(paste(c(&quot;Topic &quot;, top, &quot;: &quot;, vals), collapse=&quot;&quot;)) } topicNum = 8 idTest = &quot;1864-08-31_orders_74&quot; topicVar(topicNum, top_terms) ## Topic 8: treasury, bonds, notes, 1864, states, cent, certificates, confederate, secretary, department, america, richmond, payment, interest, treasurer print(&quot;---&quot;) ## [1] &quot;---&quot; as.String(d1864[d1864$id==idTest, ][&quot;text&quot;]) ## Treasury Department, Confederate States of America, Richmond, August 8, 1864. Certificates of Indebtedness Bearing Six Per Cent. Per Annum interest and Free from Taxation. -- By the fourteenth section of the act to reduce the currency, approved February 17, 1864, the Secretary of the Treasury is authorized to issue the above certificates, payable two years after the ratification of a treaty of peace with the United States. They cannot be sold, but are only to be issued to such creditors of the Government as are willing to receive the same in payment of their demands. They must also be given at par, though free from taxation. The attention of purchasing agents and disbursing officers of the Government is called to this class of public securities as offering peculiar advantages to those from whom the supplies of the Government are bought; and to facilitate the use of them, checks drawn by disbursing officers upon the depositaries holding these funds, and marked across the face &quot; payable in certificates of indebtedness,&quot; will be paid in conformity therewith. Depositaries are hereby authorized and required to comply with this regulation, and to make application to the Register for supplies of certificates as required. Signed) G. A. Trenholm, Secretary of Treasury. au 22 -- ts 9.9.1 Topics over time corpus_light &lt;- corpus %&gt;% select(-header, -text) lda_model_topics &lt;- lda_model_topic_doc_prob %&gt;% rename(id=document) %&gt;% left_join(corpus_light, by=&quot;id&quot;) %&gt;% group_by(topic, date) %&gt;% summarize(freq=sum(gamma)) ## `summarise()` has grouped output by &#39;topic&#39;. You can override using the ## `.groups` argument. lda_model_topics ## # A tibble: 7,725 × 3 ## # Groups: topic [25] ## topic date freq ## &lt;int&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 1864-01-01 0.219 ## 2 1 1864-01-02 0.731 ## 3 1 1864-01-04 1.02 ## 4 1 1864-01-05 0.587 ## 5 1 1864-01-06 0.708 ## 6 1 1864-01-07 1.35 ## 7 1 1864-01-08 0.400 ## 8 1 1864-01-09 0.580 ## 9 1 1864-01-11 0.838 ## 10 1 1864-01-12 0.783 ## # … with 7,715 more rows Now, we can plot topic distribution over time: topicVal = 8 lda_model_topics_final &lt;- lda_model_topics %&gt;% filter(topic == topicVal) plot(x=lda_model_topics_final$date, y=lda_model_topics_final$freq, type=&quot;l&quot;, lty=3, lwd=1, main=topicVar(topicVal, top_terms), xlab = &quot;1864 - Dispatch coverage&quot;, ylab = &quot;topic saliency&quot;) segments(x0=lda_model_topics_final$date, x1=lda_model_topics_final$date, y0=0, y1=lda_model_topics_final$freq, lty=1, lwd=2) 9.9.2 Exploring topics LDAvis offers a visual browser for topics, which has already became a very popular tool for this purpose. If everything is done right, a visualization similar to the one below should open in a browser. LDAvis Browser Example. We can use the following function that extracts all needed information from a model and converts it into a format that LDAvis expects: library(LDAvis) library(slam) topicmodels2LDAvis &lt;- function(x, ...){ post &lt;- topicmodels::posterior(x) if (ncol(post[[&quot;topics&quot;]]) &lt; 3) stop(&quot;The model must contain &gt; 2 topics&quot;) mat &lt;- x@wordassignments LDAvis::createJSON( phi = post[[&quot;terms&quot;]], theta = post[[&quot;topics&quot;]], vocab = colnames(post[[&quot;terms&quot;]]), doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE) ) } serVis(topicmodels2LDAvis(lda_model)) NB: there are some issues with LDAvis and for some reason it does not always parse out a topic model object. We can try loading another one, which does work: this is a 20 topic model based on issues of the Dispatch covering the period of 1861-1864. load(file=&quot;./files/data/dispatch_lda_vem_better.rda&quot;) serVis(topicmodels2LDAvis(dispatch_lda_vem_better)) 9.10 Addendum: different distances code sample This is simply a chunk of code that you can reuse for generating different distances. This code will not run because there is no variable YOUR_MATRIX! #library(stylo) # USING library(stylo) FUNCTIONS if (distanceMethod == &quot;cosine&quot;){distanceMatrix &lt;- dist.cosine(YOUR_MATRIX) } else if (distanceMethod == &quot;delta&quot;){distanceMatrix &lt;- dist.delta(YOUR_MATRIX) } else if (distanceMethod == &quot;argamon&quot;){distanceMatrix &lt;- dist.argamon(YOUR_MATRIX) } else if (distanceMethod == &quot;eder&quot;){distanceMatrix &lt;- dist.eder(YOUR_MATRIX) } else if (distanceMethod == &quot;minmax&quot;){distanceMatrix &lt;- dist.minmax(YOUR_MATRIX) } else if (distanceMethod == &quot;enthropy&quot;){distanceMatrix &lt;- dist.enthropy(YOUR_MATRIX) } else if (distanceMethod == &quot;simple&quot;){distanceMatrix &lt;- dist.simple(YOUR_MATRIX) } else if (distanceMethod == &quot;wurzburg&quot;){distanceMatrix &lt;- dist.wurzburg(YOUR_MATRIX) # USING dist() FUNCTION } else if (distanceMethod == &quot;euclidean&quot;){distanceMatrix &lt;- dist(YOUR_MATRIX, method=&quot;euclidean&quot;) } else if (distanceMethod == &quot;maximum&quot;){distanceMatrix &lt;- dist(YOUR_MATRIX, method=&quot;maximum&quot;) } else if (distanceMethod == &quot;manhattan&quot;){distanceMatrix &lt;- dist(YOUR_MATRIX, method=&quot;manhattan&quot;) } else if (distanceMethod == &quot;canberra&quot;){distanceMatrix &lt;- dist(YOUR_MATRIX, method=&quot;canberra&quot;) } else if (distanceMethod == &quot;binary&quot;){distanceMatrix &lt;- dist(YOUR_MATRIX, method=&quot;binary&quot;) } else if (distanceMethod == &quot;minkowski&quot;){distanceMatrix &lt;- dist(YOUR_MATRIX, method=&quot;minkowski&quot;) distanceMatrix &lt;- as.dist(distanceMatrix) clustered.data &lt;- hclust(distanceMatrix, method = clusteringMethod) … 9.11 Homework given in the chapter. 9.12 Submitting homework Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: 57528-LXX-HW-YourLastName-YourMatriculationNumber, where LXX is the number of the lesson for which you submit homework; YourLastName is your last name; and YourMatriculationNumber is your matriculation number. 9.13 Additional Materials Chapter 3 “Analyzing word and document frequency: tf-idf” in: Silge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing Boston Farnham: O´Reilly. https://www.tidytextmining.com/. Available online: https://www.tidytextmining.com/tfidf.html Chapter 6 “Topic modeling” in: Silge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. First edition. Beijing Boston Farnham: O´Reilly. https://www.tidytextmining.com/. Available online: https://www.tidytextmining.com/topicmodeling.html Chapter on Topic Modeling in: Benjamin Soltoff. MACS 305001: Computing for the Social Sciences, University of Chicago, https://cfss.uchicago.edu/notes/topic-modeling/ David Meza. Topic Modeling using R, https://knowledger.rbind.io/post/topic-modeling-using-r/ Grün, Bettina, and Kurt Hornik. 2011. “Topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13). https://doi.org/10.18637/jss.v040.i13. "],["references.html", "References", " References "]]
