# Text Analysis I: Basics

## Goals{#goals07}

- introduce basic text analysis concepts:
  - ...
  - ...
  - ...

## Preliminaries{#Prelim07}

### Data{#Data07}

We will use the following text files in this worksheet. Please download them and keep them close to your worksheet. Since some of the files are quite large, you want to download them before loading them into R:

* [The Richmond Dispatch (1862)](./files/data/dispatch_1862.tsv)
* [Star Wars I: The Phantom Menace (script)](./files/data/sw1.md)

In order to make loading these files a little bit easier, you can paste the path to where you placed these files into an isolated variable and then reuse it as follows (in other words, make sure that your `pathToFiles` is the path on your local machine):

```{r}
pathToFiles = "./files/data/"

d1862 <- read.delim(paste0(pathToFiles, "dispatch_1862.tsv"), encoding="UTF-8", header=TRUE, quote="")
sw1 <- scan(paste0(pathToFiles, "sw1.md"), what="character", sep="\n")

```

The first file is articles from "The Daily Dispatch" for the year 1862. The newspaper was published in Richmond, VA --- the capital of the Confederate States (the South) during the American Civil War (1861-1865). The second file is a script of the first episode of Star Wars :).

### Libraries{#Lib07}

The following are the libraries that we will need for this section. Install those that you do not have yet.

```{r message=FALSE}
#install.packages("tidyverse", "readr", "stringr")
#install.packages("tidytext", "wordcloud", "RColorBrewer"", "quanteda", "readtext")

# General ones 
library(tidyverse)
library(readr)
library("RColorBrewer")

# Text Analysis Specific
library(stringr)
library(tidytext)
library(wordcloud)
library(quanteda)
library(readtext)
```
### Functions in `R` (a refresher){#functions07}

Functions are groups of related statements that perform a specific task, which help breaking a program into smaller and modular chunks. As programs grow larger and larger, functions make them more organized and manageable. Functions help avoiding repetition and makes code reusable.

Most programming languages, `R` including, come with a lot of pre-defined—or built-in—functions. Essentially, all statements that take arguments in parentheses are functions. For instance, in the code chunk above, `read.delim()` is a function that takes as its arguments: 1) filename (or, path to a file); 2) encoding; 3) specifies that the file has a header; and 4) not using `"` as a special character. We can also write our own functions, which take care of sets of operations thet we tend to repeat again and again. 

Later, take a look at this [video by one of the key `R` developers](https://campus.datacamp.com/courses/writing-functions-in-r/a-quick-refresher?ex=1), and check this [tutorial](https://rpubs.com/williamsurles/292234).

#### Simple Function Example: Hypothenuse{#Hypothenuse07}

(From [Wikipedia](https://en.wikipedia.org/wiki/Hypotenuse)) In geometry, a *hypotenuse* is the longest side of a right-angled triangle, the side opposite the right angle. The length of the hypotenuse of a right triangle can be found using the Pythagorean theorem, which states that the square of the length of the hypotenuse equals the sum of the squares of the lengths of the other two sides (*catheti*). For example, if one of the other sides has a length of 3 (when squared, 9) and the other has a length of 4 (when squared, 16), then their squares add up to 25. The length of the hypotenuse is the square root of 25, that is, 5.

Let's write a function that takes lengths of catheti as arguments and returns the length of hypothenuse:

```{r}
hypothenuse <- function(cathetus1, cathetus2) {
  hypothenuse<- sqrt(cathetus1*cathetus1+cathetus2*cathetus2)
  print(paste0("In the triangle with catheti of length ",
              cathetus1, " and ", cathetus2,
              ", the length of hypothenuse is ",
              hypothenuse))
  return(hypothenuse)
}
```


Let's try a simple example:

```{r}
hypothenuse(3,4)
```

Let's try a crazy example:

```{r}
hypothenuse(390,456)
```

###$ More complex one: Cleaning Text

Let's say we want to clean up a text so that it is easier to analyze it: 1) convert everithing to lower case; 2) remove all non-alphanumeric characters; and 3) make sure that there are no multiple spaces: 

```{r}
clean_up_text = function(x) {
  x %>% 
    str_to_lower %>% # make text lower case
    str_replace_all("[^[:alnum:]]", " ") %>% # remove non-alphanumeric symbols
    str_replace_all("\\s+", " ") # collapse multiple spaces
}
```

Let's test it now:

```{r}
text = "This is a sentence with punctuation, which mentions Hamburg, a city in Germany."
clean_up_text(text)
```

## Texts and Text Analysis

We can think of text analysis as means of extracting meaningful information from structured and unstructured texts. As historians, we often do that by reading texts and collecting relevant information by taking notes, writing index cards, summarizing texts, juxtaposing one texts against another, comparing texts, looking into how specific words and terms are used, etc. Doing text analysis computationally we do lots of similar things: we extract information of specific kind, we compare texts, we look for similarities, we look differences, etc.

While there are similarities between traditional text analysis, there are of course, also significant differences. One of them is procedural: in computational reading we must explicitly perform every step of our analyses. For example, when we read a sentence, we, sort of, automatically identify the meaningful words --- subject, verb, object, etc.; we identify keywords; we parse every word, identifying what part of speech it is, what is its lemma (i.e. its dictionary form, etc.). By doing these steps we re-construct the meaning of the text that we read --- but we do most of these steps almost unconsciously, especially if a text is written in our native tongues. In computational analysis, these steps must be performed explicitly (*in the order of growing complexity*):

1. **Tokenization**: what we see as a text made of words, the computer sees as a continuous string of characters (white spaces, punctuation and the like are characters). We need to break such strings into discreet objects that computer can understand construe as words.
3. **Lemmatization**: reduces the variety of forms of the same words to their dictionary forms. Another, somewhat similar procedure is called `stemming`, which usually means the removal of most common suffixes and endings to get to the *stem* (or, *root*) of the word.
4. **POS (part-of-speech tagging)**: this is where we run some NLP tool that identifies the part of speech of each word in our text.
5. **Syntactic analysis**: is the most complicated procedure, which is also usually performed with some NLP tool, which analyzes syntactic relationships within each sentence, identifying its subject(s), verb(s), object(s), etc. 

**NOTE:**

- NLP: *natural language processing*;
- Token: you can think of *token* as a continuous string of letter characters, as a word as it appears in the text in its inflected forms with possible other attached elements (in arabic we often have prepositions, articles, pronominal suffixes, which are not part of the word, but attached to it);
- Lemma: the dictionary form of the word;
- Stem: a “root” of the word;

Some examples:

```{r message=TRUE, warning=FALSE}
#install.packages("textstem")
library(textstem)

sentence = c(
  "He tried to open one of the bigger boxes.",
  "The smaller boxes did not want to be opened.",
  "Different forms: open, opens, opened, opening, opened, opener, openers."
  )
```

The library `textstem` does lemmatization and stemming, but only for English. Tokenization can be performed with `str_split()` function --- and you can define *how* you want your string to be split.

- Tokenization

```{r}
str_split(sentence, "\\W+")
```

- Lemmatization

```{r}
lemmatize_strings(sentence)
```

- Stemming

```{r}
stem_strings(sentence)
```

*Note:* It is often important to ensure that all capital letters are converted into small letters or the other way around; additionally, some *normalization* procedures may be necessary to reduce orthographic complexities of specific languages (for example, ö > oe in German; simplification of forms of *alif* in Arabic).


## Homework{#HW07}

- Finish your worksheet and submit your HW as described below.
* *Additional:* if you'd like more practice, you can use `swirl` library:
  * To install: `install.packages("swirl")`
  * To run: `library(swirl)`
    * Then: `swirl()`
    * it will offer you a set of interactive exercises similar to DataCamp.

## Submitting homework{#SHW07}

* Homework assignment must be submitted by the beginning of the next class;
* Email your homework to the instructor as attachments.
	*  In the subject of your email, please, add the following: `57528-LXX-HW-YourLastName-YourMatriculationNumber`, where `LXX` is the number of the lesson for which you submit homework; `YourLastName` is your last name; and `YourMatriculationNumber` is your matriculation number.
	
**NB:** The original version has beed developed and prepared by Maxim Romanov for the course "R for Historical Research" (U Vienna, Spring 2019).